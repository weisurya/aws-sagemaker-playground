{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "\n",
    "# This code is derived from AWS SageMaker Samples:\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a good base job name when building different models\n",
    "# It will help in identifying trained models and endpoints\n",
    "with_categories = True\n",
    "if with_categories:\n",
    "    base_job_name = 'deepar-biketrain-with-categories'\n",
    "else:\n",
    "    base_job_name = 'deepar-biketrain-no-categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'weisurya-sagemaker-playground'\n",
    "prefix = 'deepar/bikerental'\n",
    "\n",
    "# This structure allows multiple training and test files for model development and testing\n",
    "if with_categories:\n",
    "    s3_data_path = \"{}/{}/data_with_categories\".format(bucket, prefix)\n",
    "else:\n",
    "    s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "    \n",
    "\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('weisurya-sagemaker-playground/deepar/bikerental/data_with_categories',\n",
       " 'weisurya-sagemaker-playground/deepar/bikerental/output')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path,s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name is referred as key name in S3\n",
    "# Files stored in S3 are automatically replicated across\n",
    "# three different availability zones in the region where the bucket was created.\n",
    "# http://boto3.readthedocs.io/en/latest/guide/s3.html\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload one or more training files and test files to S3\n",
    "if with_categories:\n",
    "    write_to_s3('train_with_categories.json',bucket,'deepar/bikerental/data_with_categories/train/train_with_categories.json')\n",
    "    write_to_s3('test_with_categories.json',bucket,'deepar/bikerental/data_with_categories/test/test_with_categories.json')\n",
    "else:\n",
    "    write_to_s3('train.json',bucket,'deepar/bikerental/data/train/train.json')\n",
    "    write_to_s3('test.json',bucket,'deepar/bikerental/data/test/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer have to maintain a mapping of container images by region\n",
    "# Simply use the convenience method provided by sagemaker\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'156387875391.dkr.ecr.us-west-2.amazonaws.com/forecasting-deepar:1'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='H' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 288 \n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Free Tier (if you are still under free-tier)\n",
    "# At this time, m4.xlarge is offered as part of 2 months free tier\n",
    "# https://aws.amazon.com/sagemaker/pricing/\n",
    "# If you are outside of free-tier, you can also use ml.m5.xlarge  (newer generation instance)\n",
    "# In this example, I am using ml.m5.xlarge for training\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    base_job_name=base_job_name,\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('H', 288, 288)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_freq': 'H',\n",
       " 'epochs': '400',\n",
       " 'early_stopping_patience': '40',\n",
       " 'mini_batch_size': '64',\n",
       " 'learning_rate': '5E-4',\n",
       " 'context_length': '288',\n",
       " 'prediction_length': '288',\n",
       " 'cardinality': 'auto'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are simply referring to train path and test path\n",
    "# You can have multiple files in each path\n",
    "# SageMaker will use all the files\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://weisurya-sagemaker-playground/deepar/bikerental/data_with_categories/train/',\n",
       " 'test': 's3://weisurya-sagemaker-playground/deepar/bikerental/data_with_categories/test/'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-29 03:55:20 Starting - Starting the training job...\n",
      "2020-01-29 03:55:21 Starting - Launching requested ML instances......\n",
      "2020-01-29 03:56:23 Starting - Preparing the instances for training...\n",
      "2020-01-29 03:57:04 Downloading - Downloading input data...\n",
      "2020-01-29 03:57:36 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'mini_batch_size': u'64', u'learning_rate': u'5E-4', u'prediction_length': u'288', u'epochs': u'400', u'time_freq': u'H', u'context_length': u'288', u'cardinality': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'288', u'time_freq': u'H', u'context_length': u'288', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train_with_categories.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train_with_categories.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] [cardinality=auto] Inferred value of cardinality=[3] from dataset.\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Training set statistics:\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Integer time series\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] number of time series: 3\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] number of observations: 50904\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] mean target length: 16968\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] min/mean/max target: 0.0/79.5729608675/977.0\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] mean abs(target): 79.5729608675\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Test set statistics:\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Integer time series\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] number of time series: 3\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] number of observations: 51768\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] mean target length: 17256\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] min/mean/max target: 0.0/80.5700819039/977.0\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] mean abs(target): 80.5700819039\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] contains missing values: yes (36.9%)\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] nvidia-smi took: 0.0251350402832 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:57:38 INFO 139797266573120] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 24208.503007888794, \"sum\": 24208.503007888794, \"min\": 24208.503007888794}}, \"EndTime\": 1580270282.982541, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270258.77314}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:02 INFO 139797266573120] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 27152.21095085144, \"sum\": 27152.21095085144, \"min\": 27152.21095085144}}, \"EndTime\": 1580270285.925477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270282.983009}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:08 INFO 139797266573120] Epoch[0] Batch[0] avg_epoch_loss=4.121354\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:08 INFO 139797266573120] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.12135362625\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:13 INFO 139797266573120] Epoch[0] Batch[5] avg_epoch_loss=3.930306\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:13 INFO 139797266573120] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.93030603727\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:13 INFO 139797266573120] Epoch[0] Batch [5]#011Speed: 59.42 samples/sec#011loss=3.930306\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:17 INFO 139797266573120] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 11699.819087982178, \"sum\": 11699.819087982178, \"min\": 11699.819087982178}}, \"EndTime\": 1580270297.625524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270285.925611}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:17 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.0172468005 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:17 INFO 139797266573120] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.84125628471\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:17 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:17 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_25374ed8-e4b1-42a1-a529-b765c46c1112-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 228.0271053314209, \"sum\": 228.0271053314209, \"min\": 228.0271053314209}}, \"EndTime\": 1580270297.854077, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270297.62564}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:19 INFO 139797266573120] Epoch[1] Batch[0] avg_epoch_loss=3.556341\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:19 INFO 139797266573120] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.55634117126\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:24 INFO 139797266573120] Epoch[1] Batch[5] avg_epoch_loss=3.643616\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.64361560345\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:24 INFO 139797266573120] Epoch[1] Batch [5]#011Speed: 61.76 samples/sec#011loss=3.643616\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:28 INFO 139797266573120] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10957.689046859741, \"sum\": 10957.689046859741, \"min\": 10957.689046859741}}, \"EndTime\": 1580270308.811883, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270297.85414}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:28 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.5769296261 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:28 INFO 139797266573120] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:28 INFO 139797266573120] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.61462843418\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:28 INFO 139797266573120] best epoch loss so far\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 03:58:29 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_3be546db-c049-4d37-8d78-ed391736d62b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 212.52202987670898, \"sum\": 212.52202987670898, \"min\": 212.52202987670898}}, \"EndTime\": 1580270309.024831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270308.811948}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:30 INFO 139797266573120] Epoch[2] Batch[0] avg_epoch_loss=3.495903\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:30 INFO 139797266573120] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.4959025383\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:35 INFO 139797266573120] Epoch[2] Batch[5] avg_epoch_loss=3.570497\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:35 INFO 139797266573120] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.57049675783\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:35 INFO 139797266573120] Epoch[2] Batch [5]#011Speed: 62.57 samples/sec#011loss=3.570497\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:39 INFO 139797266573120] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10864.694118499756, \"sum\": 10864.694118499756, \"min\": 10864.694118499756}}, \"EndTime\": 1580270319.889645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270309.024894}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:39 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.5252608385 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:39 INFO 139797266573120] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.54962103367\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:39 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:40 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_0d35cd5a-3506-4bf0-a446-d0d696319a18-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 221.38190269470215, \"sum\": 221.38190269470215, \"min\": 221.38190269470215}}, \"EndTime\": 1580270320.111677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270319.889712}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:41 INFO 139797266573120] Epoch[3] Batch[0] avg_epoch_loss=3.543276\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.54327607155\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:46 INFO 139797266573120] Epoch[3] Batch[5] avg_epoch_loss=3.431921\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.43192068736\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:46 INFO 139797266573120] Epoch[3] Batch [5]#011Speed: 62.92 samples/sec#011loss=3.431921\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:50 INFO 139797266573120] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10874.291896820068, \"sum\": 10874.291896820068, \"min\": 10874.291896820068}}, \"EndTime\": 1580270330.986112, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270320.111751}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:50 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.9342047105 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:50 INFO 139797266573120] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:50 INFO 139797266573120] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.40745177269\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:50 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:51 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_84b7e70d-fa81-4a56-9f5c-07727e6196c1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 208.0850601196289, \"sum\": 208.0850601196289, \"min\": 208.0850601196289}}, \"EndTime\": 1580270331.194771, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270330.986187}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:52 INFO 139797266573120] Epoch[4] Batch[0] avg_epoch_loss=3.417646\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:52 INFO 139797266573120] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=3.41764569283\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:57 INFO 139797266573120] Epoch[4] Batch[5] avg_epoch_loss=3.353151\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:57 INFO 139797266573120] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.35315112273\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:58:57 INFO 139797266573120] Epoch[4] Batch [5]#011Speed: 62.68 samples/sec#011loss=3.353151\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:02 INFO 139797266573120] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10886.080026626587, \"sum\": 10886.080026626587, \"min\": 10886.080026626587}}, \"EndTime\": 1580270342.080972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270331.19483}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:02 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.5959400757 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:02 INFO 139797266573120] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=4, train loss <loss>=3.33662195206\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:02 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:02 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_43f963ca-5d99-4301-8429-f9cda4ca4af1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 216.59016609191895, \"sum\": 216.59016609191895, \"min\": 216.59016609191895}}, \"EndTime\": 1580270342.298383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270342.081038}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:04 INFO 139797266573120] Epoch[5] Batch[0] avg_epoch_loss=3.304464\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.30446434021\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:09 INFO 139797266573120] Epoch[5] Batch[5] avg_epoch_loss=3.239726\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=3.23972586791\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:09 INFO 139797266573120] Epoch[5] Batch [5]#011Speed: 62.62 samples/sec#011loss=3.239726\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] Epoch[5] Batch[10] avg_epoch_loss=3.247777\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=3.25743761063\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] Epoch[5] Batch [10]#011Speed: 61.91 samples/sec#011loss=3.257438\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11989.95590209961, \"sum\": 11989.95590209961, \"min\": 11989.95590209961}}, \"EndTime\": 1580270354.288468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270342.298445}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.7136632098 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=5, train loss <loss>=3.24777666005\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:14 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_4db68ee9-fc85-4ce3-88e5-aeb88e1c866f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 206.71701431274414, \"sum\": 206.71701431274414, \"min\": 206.71701431274414}}, \"EndTime\": 1580270354.495695, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270354.288528}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:16 INFO 139797266573120] Epoch[6] Batch[0] avg_epoch_loss=3.087533\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:16 INFO 139797266573120] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.08753347397\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 03:59:21 INFO 139797266573120] Epoch[6] Batch[5] avg_epoch_loss=3.162816\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.16281580925\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:21 INFO 139797266573120] Epoch[6] Batch [5]#011Speed: 61.97 samples/sec#011loss=3.162816\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:25 INFO 139797266573120] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10984.293937683105, \"sum\": 10984.293937683105, \"min\": 10984.293937683105}}, \"EndTime\": 1580270365.480115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270354.495764}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:25 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.079337261 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:25 INFO 139797266573120] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.10712656975\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:25 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:25 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_c1f4269e-4a97-4359-892a-b6d09ae4102c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 205.54184913635254, \"sum\": 205.54184913635254, \"min\": 205.54184913635254}}, \"EndTime\": 1580270365.686211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270365.480226}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:27 INFO 139797266573120] Epoch[7] Batch[0] avg_epoch_loss=3.114256\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.11425566673\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:32 INFO 139797266573120] Epoch[7] Batch[5] avg_epoch_loss=3.088907\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=3.08890676498\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:32 INFO 139797266573120] Epoch[7] Batch [5]#011Speed: 62.56 samples/sec#011loss=3.088907\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:36 INFO 139797266573120] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10901.880979537964, \"sum\": 10901.880979537964, \"min\": 10901.880979537964}}, \"EndTime\": 1580270376.588213, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270365.686274}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:36 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.4942423241 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:36 INFO 139797266573120] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.17414784431\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:36 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:38 INFO 139797266573120] Epoch[8] Batch[0] avg_epoch_loss=3.212585\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=3.21258544922\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:43 INFO 139797266573120] Epoch[8] Batch[5] avg_epoch_loss=3.088109\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=3.08810909589\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:43 INFO 139797266573120] Epoch[8] Batch [5]#011Speed: 63.07 samples/sec#011loss=3.088109\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:48 INFO 139797266573120] Epoch[8] Batch[10] avg_epoch_loss=3.197631\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=3.32905654907\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:48 INFO 139797266573120] Epoch[8] Batch [10]#011Speed: 62.41 samples/sec#011loss=3.329057\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:48 INFO 139797266573120] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11857.563018798828, \"sum\": 11857.563018798828, \"min\": 11857.563018798828}}, \"EndTime\": 1580270388.446235, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270376.588332}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:48 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.142216678 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:48 INFO 139797266573120] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=8, train loss <loss>=3.19763066552\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:48 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:50 INFO 139797266573120] Epoch[9] Batch[0] avg_epoch_loss=2.983736\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:50 INFO 139797266573120] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=2.98373556137\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:55 INFO 139797266573120] Epoch[9] Batch[5] avg_epoch_loss=2.959035\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=2.95903464158\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:55 INFO 139797266573120] Epoch[9] Batch [5]#011Speed: 62.40 samples/sec#011loss=2.959035\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:59 INFO 139797266573120] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10886.868000030518, \"sum\": 10886.868000030518, \"min\": 10886.868000030518}}, \"EndTime\": 1580270399.333669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270388.446298}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:59 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.4895625364 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:59 INFO 139797266573120] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:59 INFO 139797266573120] #quality_metric: host=algo-1, epoch=9, train loss <loss>=2.95363688469\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:59 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 03:59:59 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_eb346ff2-7b6a-4cc0-a2af-bba9b550539a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 214.25104141235352, \"sum\": 214.25104141235352, \"min\": 214.25104141235352}}, \"EndTime\": 1580270399.548366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270399.333735}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:01 INFO 139797266573120] Epoch[10] Batch[0] avg_epoch_loss=2.861956\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:01 INFO 139797266573120] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=2.86195635796\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:06 INFO 139797266573120] Epoch[10] Batch[5] avg_epoch_loss=2.937511\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=2.93751136462\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:06 INFO 139797266573120] Epoch[10] Batch [5]#011Speed: 62.13 samples/sec#011loss=2.937511\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:10 INFO 139797266573120] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10913.434982299805, \"sum\": 10913.434982299805, \"min\": 10913.434982299805}}, \"EndTime\": 1580270410.461922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270399.54843}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:10 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.7864508832 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:10 INFO 139797266573120] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=10, train loss <loss>=2.91724965572\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:10 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:10 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_95401e0f-3f92-45db-86c2-312cf69eb3d8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 210.21485328674316, \"sum\": 210.21485328674316, \"min\": 210.21485328674316}}, \"EndTime\": 1580270410.672646, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270410.461986}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:12 INFO 139797266573120] Epoch[11] Batch[0] avg_epoch_loss=2.961461\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=2.9614610672\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:00:17 INFO 139797266573120] Epoch[11] Batch[5] avg_epoch_loss=2.962284\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.96228408813\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:17 INFO 139797266573120] Epoch[11] Batch [5]#011Speed: 62.75 samples/sec#011loss=2.962284\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:22 INFO 139797266573120] Epoch[11] Batch[10] avg_epoch_loss=3.016880\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=3.0823946476\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:22 INFO 139797266573120] Epoch[11] Batch [10]#011Speed: 62.29 samples/sec#011loss=3.082395\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:22 INFO 139797266573120] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11894.230127334595, \"sum\": 11894.230127334595, \"min\": 11894.230127334595}}, \"EndTime\": 1580270422.566994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270410.672705}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:22 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.9001209579 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:22 INFO 139797266573120] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=11, train loss <loss>=3.01687979698\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:22 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:24 INFO 139797266573120] Epoch[12] Batch[0] avg_epoch_loss=2.859250\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.85924983025\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:29 INFO 139797266573120] Epoch[12] Batch[5] avg_epoch_loss=2.818832\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.8188324372\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:29 INFO 139797266573120] Epoch[12] Batch [5]#011Speed: 62.88 samples/sec#011loss=2.818832\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:33 INFO 139797266573120] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10872.67804145813, \"sum\": 10872.67804145813, \"min\": 10872.67804145813}}, \"EndTime\": 1580270433.440088, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270422.567058}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:33 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.8623360674 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:33 INFO 139797266573120] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:33 INFO 139797266573120] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.87616372108\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:33 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:33 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_31feff73-83e9-4126-be08-efe49a6fac3e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 219.42400932312012, \"sum\": 219.42400932312012, \"min\": 219.42400932312012}}, \"EndTime\": 1580270433.660017, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270433.440206}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:35 INFO 139797266573120] Epoch[13] Batch[0] avg_epoch_loss=2.915945\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:35 INFO 139797266573120] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.9159450531\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:40 INFO 139797266573120] Epoch[13] Batch[5] avg_epoch_loss=2.875300\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:40 INFO 139797266573120] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.87529981136\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:40 INFO 139797266573120] Epoch[13] Batch [5]#011Speed: 62.53 samples/sec#011loss=2.875300\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] Epoch[13] Batch[10] avg_epoch_loss=2.843258\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=2.8048084259\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] Epoch[13] Batch [10]#011Speed: 61.80 samples/sec#011loss=2.804808\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12009.644985198975, \"sum\": 12009.644985198975, \"min\": 12009.644985198975}}, \"EndTime\": 1580270445.669783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270433.66008}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.5374477267 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.84325827252\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:45 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_3c42491d-296e-413e-a30d-9aa93247c30b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 226.79710388183594, \"sum\": 226.79710388183594, \"min\": 226.79710388183594}}, \"EndTime\": 1580270445.897081, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270445.669846}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:47 INFO 139797266573120] Epoch[14] Batch[0] avg_epoch_loss=2.987319\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:47 INFO 139797266573120] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.98731899261\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:52 INFO 139797266573120] Epoch[14] Batch[5] avg_epoch_loss=2.829273\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:52 INFO 139797266573120] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.82927322388\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:52 INFO 139797266573120] Epoch[14] Batch [5]#011Speed: 62.12 samples/sec#011loss=2.829273\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:56 INFO 139797266573120] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10936.753988265991, \"sum\": 10936.753988265991, \"min\": 10936.753988265991}}, \"EndTime\": 1580270456.83396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270445.897143}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:56 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.7802470367 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:56 INFO 139797266573120] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.87285838127\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:56 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:58 INFO 139797266573120] Epoch[15] Batch[0] avg_epoch_loss=3.012481\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:00:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.01248121262\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:03 INFO 139797266573120] Epoch[15] Batch[5] avg_epoch_loss=2.833833\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.83383278052\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:03 INFO 139797266573120] Epoch[15] Batch [5]#011Speed: 62.17 samples/sec#011loss=2.833833\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:07 INFO 139797266573120] processed a total of 577 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10980.61203956604, \"sum\": 10980.61203956604, \"min\": 10980.61203956604}}, \"EndTime\": 1580270467.815067, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270456.834025}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:07 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=52.546650891 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:07 INFO 139797266573120] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=15, train loss <loss>=2.85103297234\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:07 INFO 139797266573120] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:01:09 INFO 139797266573120] Epoch[16] Batch[0] avg_epoch_loss=2.884969\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=2.88496923447\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:14 INFO 139797266573120] Epoch[16] Batch[5] avg_epoch_loss=2.880578\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.88057835897\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:14 INFO 139797266573120] Epoch[16] Batch [5]#011Speed: 62.09 samples/sec#011loss=2.880578\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:18 INFO 139797266573120] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10995.32699584961, \"sum\": 10995.32699584961, \"min\": 10995.32699584961}}, \"EndTime\": 1580270478.810898, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270467.815142}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:18 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.6601301257 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:18 INFO 139797266573120] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.88711965084\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:18 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:20 INFO 139797266573120] Epoch[17] Batch[0] avg_epoch_loss=2.892371\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=2.89237070084\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:25 INFO 139797266573120] Epoch[17] Batch[5] avg_epoch_loss=2.824180\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=2.82418004672\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:25 INFO 139797266573120] Epoch[17] Batch [5]#011Speed: 62.90 samples/sec#011loss=2.824180\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:29 INFO 139797266573120] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10872.673034667969, \"sum\": 10872.673034667969, \"min\": 10872.673034667969}}, \"EndTime\": 1580270489.684032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270478.811007}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:29 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.2187922498 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:29 INFO 139797266573120] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.81302976608\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:29 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:29 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_cc743bfa-8627-4862-a826-f93aa4b24049-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 224.3669033050537, \"sum\": 224.3669033050537, \"min\": 224.3669033050537}}, \"EndTime\": 1580270489.908979, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270489.684103}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:31 INFO 139797266573120] Epoch[18] Batch[0] avg_epoch_loss=2.926440\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:31 INFO 139797266573120] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.92643976212\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:36 INFO 139797266573120] Epoch[18] Batch[5] avg_epoch_loss=2.850530\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.8505303065\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:36 INFO 139797266573120] Epoch[18] Batch [5]#011Speed: 61.95 samples/sec#011loss=2.850530\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:41 INFO 139797266573120] Epoch[18] Batch[10] avg_epoch_loss=2.826679\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=2.79805831909\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:41 INFO 139797266573120] Epoch[18] Batch [10]#011Speed: 62.16 samples/sec#011loss=2.798058\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:41 INFO 139797266573120] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12004.43696975708, \"sum\": 12004.43696975708, \"min\": 12004.43696975708}}, \"EndTime\": 1580270501.913531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270489.909039}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:41 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.1461987182 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:41 INFO 139797266573120] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.82667940313\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:41 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:43 INFO 139797266573120] Epoch[19] Batch[0] avg_epoch_loss=2.731270\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.73127007484\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:48 INFO 139797266573120] Epoch[19] Batch[5] avg_epoch_loss=2.698716\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=2.69871608416\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:48 INFO 139797266573120] Epoch[19] Batch [5]#011Speed: 62.69 samples/sec#011loss=2.698716\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:52 INFO 139797266573120] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10879.002809524536, \"sum\": 10879.002809524536, \"min\": 10879.002809524536}}, \"EndTime\": 1580270512.792958, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270501.913598}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:52 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.0930568851 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:52 INFO 139797266573120] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:52 INFO 139797266573120] #quality_metric: host=algo-1, epoch=19, train loss <loss>=2.73601791859\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:52 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:53 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_65ba3acf-587e-4981-ae18-e13eb7dfb82a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 216.35198593139648, \"sum\": 216.35198593139648, \"min\": 216.35198593139648}}, \"EndTime\": 1580270513.009924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270512.793025}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:54 INFO 139797266573120] Epoch[20] Batch[0] avg_epoch_loss=2.796531\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:54 INFO 139797266573120] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.79653072357\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:59 INFO 139797266573120] Epoch[20] Batch[5] avg_epoch_loss=2.828253\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:59 INFO 139797266573120] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=2.82825318972\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:01:59 INFO 139797266573120] Epoch[20] Batch [5]#011Speed: 62.33 samples/sec#011loss=2.828253\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:05 INFO 139797266573120] Epoch[20] Batch[10] avg_epoch_loss=2.749017\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:05 INFO 139797266573120] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=2.65393462181\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:05 INFO 139797266573120] Epoch[20] Batch [10]#011Speed: 61.95 samples/sec#011loss=2.653935\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:05 INFO 139797266573120] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11992.584943771362, \"sum\": 11992.584943771362, \"min\": 11992.584943771362}}, \"EndTime\": 1580270525.002628, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270513.009983}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:05 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.6160355941 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:05 INFO 139797266573120] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:05 INFO 139797266573120] #quality_metric: host=algo-1, epoch=20, train loss <loss>=2.74901747704\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:05 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:06 INFO 139797266573120] Epoch[21] Batch[0] avg_epoch_loss=2.866474\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.86647367477\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:02:11 INFO 139797266573120] Epoch[21] Batch[5] avg_epoch_loss=2.814417\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.81441668669\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:11 INFO 139797266573120] Epoch[21] Batch [5]#011Speed: 62.20 samples/sec#011loss=2.814417\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:17 INFO 139797266573120] Epoch[21] Batch[10] avg_epoch_loss=2.825153\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=2.83803744316\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:17 INFO 139797266573120] Epoch[21] Batch [10]#011Speed: 62.10 samples/sec#011loss=2.838037\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:17 INFO 139797266573120] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12008.004903793335, \"sum\": 12008.004903793335, \"min\": 12008.004903793335}}, \"EndTime\": 1580270537.01107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270525.00269}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:17 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.1301598494 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:17 INFO 139797266573120] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.82515339418\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:17 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:18 INFO 139797266573120] Epoch[22] Batch[0] avg_epoch_loss=2.878096\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=2.87809634209\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:23 INFO 139797266573120] Epoch[22] Batch[5] avg_epoch_loss=2.814521\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=2.81452115377\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:23 INFO 139797266573120] Epoch[22] Batch [5]#011Speed: 62.67 samples/sec#011loss=2.814521\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:27 INFO 139797266573120] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10906.142950057983, \"sum\": 10906.142950057983, \"min\": 10906.142950057983}}, \"EndTime\": 1580270547.917541, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270537.011129}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:27 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.3152079182 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:27 INFO 139797266573120] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=22, train loss <loss>=2.80278377533\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:27 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:29 INFO 139797266573120] Epoch[23] Batch[0] avg_epoch_loss=2.867659\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.86765861511\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:34 INFO 139797266573120] Epoch[23] Batch[5] avg_epoch_loss=2.762104\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.76210419337\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:34 INFO 139797266573120] Epoch[23] Batch [5]#011Speed: 62.50 samples/sec#011loss=2.762104\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:39 INFO 139797266573120] Epoch[23] Batch[10] avg_epoch_loss=2.764239\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=2.76680011749\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:39 INFO 139797266573120] Epoch[23] Batch [10]#011Speed: 63.00 samples/sec#011loss=2.766800\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:39 INFO 139797266573120] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11910.505056381226, \"sum\": 11910.505056381226, \"min\": 11910.505056381226}}, \"EndTime\": 1580270559.828475, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270547.917611}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:39 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.1534704928 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:39 INFO 139797266573120] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=23, train loss <loss>=2.76423870433\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:39 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:41 INFO 139797266573120] Epoch[24] Batch[0] avg_epoch_loss=2.792177\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.79217743874\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:46 INFO 139797266573120] Epoch[24] Batch[5] avg_epoch_loss=2.757569\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=2.75756867727\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:46 INFO 139797266573120] Epoch[24] Batch [5]#011Speed: 62.63 samples/sec#011loss=2.757569\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:51 INFO 139797266573120] Epoch[24] Batch[10] avg_epoch_loss=2.778495\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=2.80360622406\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:51 INFO 139797266573120] Epoch[24] Batch [10]#011Speed: 61.91 samples/sec#011loss=2.803606\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:51 INFO 139797266573120] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11994.786977767944, \"sum\": 11994.786977767944, \"min\": 11994.786977767944}}, \"EndTime\": 1580270571.823638, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270559.828536}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:51 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.4407201896 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:51 INFO 139797266573120] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=24, train loss <loss>=2.7784948349\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:51 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:53 INFO 139797266573120] Epoch[25] Batch[0] avg_epoch_loss=2.728878\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.72887825966\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:58 INFO 139797266573120] Epoch[25] Batch[5] avg_epoch_loss=2.732836\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.73283648491\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:02:58 INFO 139797266573120] Epoch[25] Batch [5]#011Speed: 61.60 samples/sec#011loss=2.732836\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:03 INFO 139797266573120] Epoch[25] Batch[10] avg_epoch_loss=2.766661\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=2.80724945068\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:03 INFO 139797266573120] Epoch[25] Batch [10]#011Speed: 61.64 samples/sec#011loss=2.807249\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:03 INFO 139797266573120] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12124.728918075562, \"sum\": 12124.728918075562, \"min\": 12124.728918075562}}, \"EndTime\": 1580270583.948747, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270571.823706}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:03 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.4235075811 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:03 INFO 139797266573120] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=25, train loss <loss>=2.76666056026\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:03 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:05 INFO 139797266573120] Epoch[26] Batch[0] avg_epoch_loss=2.836415\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:05 INFO 139797266573120] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.83641457558\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:03:10 INFO 139797266573120] Epoch[26] Batch[5] avg_epoch_loss=2.815939\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.81593914827\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:10 INFO 139797266573120] Epoch[26] Batch [5]#011Speed: 61.75 samples/sec#011loss=2.815939\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:15 INFO 139797266573120] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11058.558940887451, \"sum\": 11058.558940887451, \"min\": 11058.558940887451}}, \"EndTime\": 1580270595.007688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270583.948809}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:15 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.3359854348 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:15 INFO 139797266573120] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:15 INFO 139797266573120] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.78786678314\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:15 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:16 INFO 139797266573120] Epoch[27] Batch[0] avg_epoch_loss=2.681638\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:16 INFO 139797266573120] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.6816380024\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:21 INFO 139797266573120] Epoch[27] Batch[5] avg_epoch_loss=2.699740\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=2.69973989328\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:21 INFO 139797266573120] Epoch[27] Batch [5]#011Speed: 61.65 samples/sec#011loss=2.699740\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:26 INFO 139797266573120] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11020.888805389404, \"sum\": 11020.888805389404, \"min\": 11020.888805389404}}, \"EndTime\": 1580270606.029112, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270595.00775}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:26 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.2561971804 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:26 INFO 139797266573120] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:26 INFO 139797266573120] #quality_metric: host=algo-1, epoch=27, train loss <loss>=2.77009625435\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:26 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:27 INFO 139797266573120] Epoch[28] Batch[0] avg_epoch_loss=2.823802\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.82380247116\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:32 INFO 139797266573120] Epoch[28] Batch[5] avg_epoch_loss=2.750278\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.75027807554\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:32 INFO 139797266573120] Epoch[28] Batch [5]#011Speed: 62.75 samples/sec#011loss=2.750278\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:38 INFO 139797266573120] Epoch[28] Batch[10] avg_epoch_loss=2.765512\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=2.7837937355\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:38 INFO 139797266573120] Epoch[28] Batch [10]#011Speed: 61.90 samples/sec#011loss=2.783794\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:38 INFO 139797266573120] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11973.047971725464, \"sum\": 11973.047971725464, \"min\": 11973.047971725464}}, \"EndTime\": 1580270618.002744, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270606.029191}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:38 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.288082253 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:38 INFO 139797266573120] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.76551246643\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:38 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:39 INFO 139797266573120] Epoch[29] Batch[0] avg_epoch_loss=2.702656\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.70265579224\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:44 INFO 139797266573120] Epoch[29] Batch[5] avg_epoch_loss=2.666198\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.66619817416\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:44 INFO 139797266573120] Epoch[29] Batch [5]#011Speed: 62.01 samples/sec#011loss=2.666198\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] Epoch[29] Batch[10] avg_epoch_loss=2.649964\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=2.6304831028\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] Epoch[29] Batch [10]#011Speed: 61.80 samples/sec#011loss=2.630483\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12023.11396598816, \"sum\": 12023.11396598816, \"min\": 12023.11396598816}}, \"EndTime\": 1580270630.026329, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270618.002826}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.2265486773 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.64996405081\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:50 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_c02bf435-c618-4cc7-aa07-6d1a3b878d6f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 218.54090690612793, \"sum\": 218.54090690612793, \"min\": 218.54090690612793}}, \"EndTime\": 1580270630.245435, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270630.02639}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:51 INFO 139797266573120] Epoch[30] Batch[0] avg_epoch_loss=2.738219\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.73821854591\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:57 INFO 139797266573120] Epoch[30] Batch[5] avg_epoch_loss=2.654441\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:57 INFO 139797266573120] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.65444123745\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:03:57 INFO 139797266573120] Epoch[30] Batch [5]#011Speed: 62.27 samples/sec#011loss=2.654441\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] Epoch[30] Batch[10] avg_epoch_loss=2.622432\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=2.58402171135\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] Epoch[30] Batch [10]#011Speed: 61.78 samples/sec#011loss=2.584022\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12037.294149398804, \"sum\": 12037.294149398804, \"min\": 12037.294149398804}}, \"EndTime\": 1580270642.282843, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270630.245493}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.6599052687 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.62243236195\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:02 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_fd411309-f053-4521-840e-5c0c006b0155-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 219.49100494384766, \"sum\": 219.49100494384766, \"min\": 219.49100494384766}}, \"EndTime\": 1580270642.502785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270642.282911}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:04 INFO 139797266573120] Epoch[31] Batch[0] avg_epoch_loss=2.815070\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.81506991386\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:04:09 INFO 139797266573120] Epoch[31] Batch[5] avg_epoch_loss=2.699894\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.69989399115\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:09 INFO 139797266573120] Epoch[31] Batch [5]#011Speed: 61.76 samples/sec#011loss=2.699894\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:13 INFO 139797266573120] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10994.853973388672, \"sum\": 10994.853973388672, \"min\": 10994.853973388672}}, \"EndTime\": 1580270653.497783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270642.50285}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:13 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.8425232139 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:13 INFO 139797266573120] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:13 INFO 139797266573120] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.79720900059\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:13 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:15 INFO 139797266573120] Epoch[32] Batch[0] avg_epoch_loss=2.674950\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:15 INFO 139797266573120] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.67494988441\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:20 INFO 139797266573120] Epoch[32] Batch[5] avg_epoch_loss=2.675197\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.67519728343\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:20 INFO 139797266573120] Epoch[32] Batch [5]#011Speed: 62.04 samples/sec#011loss=2.675197\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:25 INFO 139797266573120] Epoch[32] Batch[10] avg_epoch_loss=2.755021\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=2.85081038475\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:25 INFO 139797266573120] Epoch[32] Batch [10]#011Speed: 61.51 samples/sec#011loss=2.850810\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:25 INFO 139797266573120] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12065.131902694702, \"sum\": 12065.131902694702, \"min\": 12065.131902694702}}, \"EndTime\": 1580270665.563423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270653.497905}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:25 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.7854238068 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:25 INFO 139797266573120] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.75502142039\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:25 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:27 INFO 139797266573120] Epoch[33] Batch[0] avg_epoch_loss=2.656154\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.65615367889\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:32 INFO 139797266573120] Epoch[33] Batch[5] avg_epoch_loss=2.662140\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.66213961442\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:32 INFO 139797266573120] Epoch[33] Batch [5]#011Speed: 62.20 samples/sec#011loss=2.662140\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:37 INFO 139797266573120] Epoch[33] Batch[10] avg_epoch_loss=2.642741\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.61946249008\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:37 INFO 139797266573120] Epoch[33] Batch [10]#011Speed: 61.94 samples/sec#011loss=2.619462\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:37 INFO 139797266573120] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12059.498071670532, \"sum\": 12059.498071670532, \"min\": 12059.498071670532}}, \"EndTime\": 1580270677.623299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270665.563515}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:37 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.9720704597 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:37 INFO 139797266573120] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.64274092154\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:37 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:39 INFO 139797266573120] Epoch[34] Batch[0] avg_epoch_loss=2.576624\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.57662415504\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:44 INFO 139797266573120] Epoch[34] Batch[5] avg_epoch_loss=2.649050\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.64904971917\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:44 INFO 139797266573120] Epoch[34] Batch [5]#011Speed: 62.16 samples/sec#011loss=2.649050\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] Epoch[34] Batch[10] avg_epoch_loss=2.602995\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=2.54772973061\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] Epoch[34] Batch [10]#011Speed: 62.18 samples/sec#011loss=2.547730\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11987.038850784302, \"sum\": 11987.038850784302, \"min\": 11987.038850784302}}, \"EndTime\": 1580270689.610855, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270677.623358}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.8921869989 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.60299517892\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:49 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_610c34b8-2052-430a-97c5-103fb300da0a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 214.1098976135254, \"sum\": 214.1098976135254, \"min\": 214.1098976135254}}, \"EndTime\": 1580270689.825438, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270689.610919}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:51 INFO 139797266573120] Epoch[35] Batch[0] avg_epoch_loss=2.578201\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.57820129395\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:56 INFO 139797266573120] Epoch[35] Batch[5] avg_epoch_loss=2.633937\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.63393668334\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:04:56 INFO 139797266573120] Epoch[35] Batch [5]#011Speed: 62.48 samples/sec#011loss=2.633937\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:00 INFO 139797266573120] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10908.648014068604, \"sum\": 10908.648014068604, \"min\": 10908.648014068604}}, \"EndTime\": 1580270700.734209, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270689.8255}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:00 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.1100453305 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:00 INFO 139797266573120] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.61749491692\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:00 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:02 INFO 139797266573120] Epoch[36] Batch[0] avg_epoch_loss=2.691973\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.69197320938\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:07 INFO 139797266573120] Epoch[36] Batch[5] avg_epoch_loss=2.687251\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.68725085258\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:07 INFO 139797266573120] Epoch[36] Batch [5]#011Speed: 62.58 samples/sec#011loss=2.687251\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:11 INFO 139797266573120] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10922.31011390686, \"sum\": 10922.31011390686, \"min\": 10922.31011390686}}, \"EndTime\": 1580270711.657022, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270700.734289}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:11 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.404865709 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:11 INFO 139797266573120] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.66667385101\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:11 INFO 139797266573120] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:05:13 INFO 139797266573120] Epoch[37] Batch[0] avg_epoch_loss=2.539564\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:13 INFO 139797266573120] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.53956365585\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:18 INFO 139797266573120] Epoch[37] Batch[5] avg_epoch_loss=2.625996\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.62599591414\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:18 INFO 139797266573120] Epoch[37] Batch [5]#011Speed: 62.44 samples/sec#011loss=2.625996\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:22 INFO 139797266573120] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10983.40392112732, \"sum\": 10983.40392112732, \"min\": 10983.40392112732}}, \"EndTime\": 1580270722.640997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270711.657101}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:22 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.7207956922 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:22 INFO 139797266573120] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.60754654408\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:22 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:24 INFO 139797266573120] Epoch[38] Batch[0] avg_epoch_loss=2.624972\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.62497234344\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:29 INFO 139797266573120] Epoch[38] Batch[5] avg_epoch_loss=2.603510\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.60351002216\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:29 INFO 139797266573120] Epoch[38] Batch [5]#011Speed: 62.12 samples/sec#011loss=2.603510\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] Epoch[38] Batch[10] avg_epoch_loss=2.558144\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=2.50370469093\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] Epoch[38] Batch [10]#011Speed: 62.22 samples/sec#011loss=2.503705\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11973.172903060913, \"sum\": 11973.172903060913, \"min\": 11973.172903060913}}, \"EndTime\": 1580270734.614825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270722.641064}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.9557103319 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.55814396251\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:34 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_5fd6100a-d119-401b-8a1f-dc9eb88d2d7e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 206.83789253234863, \"sum\": 206.83789253234863, \"min\": 206.83789253234863}}, \"EndTime\": 1580270734.822186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270734.614897}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:36 INFO 139797266573120] Epoch[39] Batch[0] avg_epoch_loss=2.722350\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.72234964371\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:41 INFO 139797266573120] Epoch[39] Batch[5] avg_epoch_loss=2.654728\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.65472813447\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:41 INFO 139797266573120] Epoch[39] Batch [5]#011Speed: 62.64 samples/sec#011loss=2.654728\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:46 INFO 139797266573120] Epoch[39] Batch[10] avg_epoch_loss=2.654750\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=2.65477657318\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:46 INFO 139797266573120] Epoch[39] Batch [10]#011Speed: 62.47 samples/sec#011loss=2.654777\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:46 INFO 139797266573120] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11924.965858459473, \"sum\": 11924.965858459473, \"min\": 11924.965858459473}}, \"EndTime\": 1580270746.747273, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270734.822249}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:46 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.3582109829 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:46 INFO 139797266573120] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=39, train loss <loss>=2.65475015207\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:46 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:48 INFO 139797266573120] Epoch[40] Batch[0] avg_epoch_loss=2.734593\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.73459267616\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:53 INFO 139797266573120] Epoch[40] Batch[5] avg_epoch_loss=2.666569\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.66656923294\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:53 INFO 139797266573120] Epoch[40] Batch [5]#011Speed: 62.65 samples/sec#011loss=2.666569\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:57 INFO 139797266573120] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10898.321866989136, \"sum\": 10898.321866989136, \"min\": 10898.321866989136}}, \"EndTime\": 1580270757.645929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270746.747335}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:57 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.3377855517 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:57 INFO 139797266573120] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:57 INFO 139797266573120] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.68211913109\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:57 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:59 INFO 139797266573120] Epoch[41] Batch[0] avg_epoch_loss=2.652234\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:05:59 INFO 139797266573120] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.65223407745\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:04 INFO 139797266573120] Epoch[41] Batch[5] avg_epoch_loss=2.627734\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.62773362796\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:04 INFO 139797266573120] Epoch[41] Batch [5]#011Speed: 62.30 samples/sec#011loss=2.627734\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:09 INFO 139797266573120] Epoch[41] Batch[10] avg_epoch_loss=2.653086\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=2.68350930214\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:09 INFO 139797266573120] Epoch[41] Batch [10]#011Speed: 61.33 samples/sec#011loss=2.683509\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:09 INFO 139797266573120] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12043.403148651123, \"sum\": 12043.403148651123, \"min\": 12043.403148651123}}, \"EndTime\": 1580270769.689979, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270757.646121}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:09 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.7183260242 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:09 INFO 139797266573120] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.65308620713\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:09 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:11 INFO 139797266573120] Epoch[42] Batch[0] avg_epoch_loss=2.618980\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.61897993088\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:06:16 INFO 139797266573120] Epoch[42] Batch[5] avg_epoch_loss=2.660656\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:16 INFO 139797266573120] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.66065565745\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:16 INFO 139797266573120] Epoch[42] Batch [5]#011Speed: 60.30 samples/sec#011loss=2.660656\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:21 INFO 139797266573120] Epoch[42] Batch[10] avg_epoch_loss=2.693820\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=2.73361654282\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:21 INFO 139797266573120] Epoch[42] Batch [10]#011Speed: 61.58 samples/sec#011loss=2.733617\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:21 INFO 139797266573120] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12219.67101097107, \"sum\": 12219.67101097107, \"min\": 12219.67101097107}}, \"EndTime\": 1580270781.910006, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270769.690042}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:21 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.8292362377 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:21 INFO 139797266573120] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.69381969625\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:21 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:23 INFO 139797266573120] Epoch[43] Batch[0] avg_epoch_loss=2.544767\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.54476690292\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:28 INFO 139797266573120] Epoch[43] Batch[5] avg_epoch_loss=2.647528\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:28 INFO 139797266573120] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.64752837022\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:28 INFO 139797266573120] Epoch[43] Batch [5]#011Speed: 62.34 samples/sec#011loss=2.647528\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:32 INFO 139797266573120] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10945.954084396362, \"sum\": 10945.954084396362, \"min\": 10945.954084396362}}, \"EndTime\": 1580270792.856424, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270781.910063}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:32 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.7327733842 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:32 INFO 139797266573120] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.64127047062\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:32 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:34 INFO 139797266573120] Epoch[44] Batch[0] avg_epoch_loss=2.770614\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.77061414719\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:39 INFO 139797266573120] Epoch[44] Batch[5] avg_epoch_loss=2.712198\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.7121976614\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:39 INFO 139797266573120] Epoch[44] Batch [5]#011Speed: 62.66 samples/sec#011loss=2.712198\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:44 INFO 139797266573120] Epoch[44] Batch[10] avg_epoch_loss=2.759973\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=2.81730260849\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:44 INFO 139797266573120] Epoch[44] Batch [10]#011Speed: 62.34 samples/sec#011loss=2.817303\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:44 INFO 139797266573120] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11951.091051101685, \"sum\": 11951.091051101685, \"min\": 11951.091051101685}}, \"EndTime\": 1580270804.808158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270792.856489}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:44 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.2245986485 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:44 INFO 139797266573120] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.75997263735\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:44 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:46 INFO 139797266573120] Epoch[45] Batch[0] avg_epoch_loss=2.442159\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.44215917587\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:51 INFO 139797266573120] Epoch[45] Batch[5] avg_epoch_loss=2.602035\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.60203456879\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:51 INFO 139797266573120] Epoch[45] Batch [5]#011Speed: 63.05 samples/sec#011loss=2.602035\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:55 INFO 139797266573120] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10834.83600616455, \"sum\": 10834.83600616455, \"min\": 10834.83600616455}}, \"EndTime\": 1580270815.643456, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270804.808228}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:55 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.7760991085 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:55 INFO 139797266573120] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.62013030052\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:55 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:57 INFO 139797266573120] Epoch[46] Batch[0] avg_epoch_loss=2.689747\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:06:57 INFO 139797266573120] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.68974661827\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:02 INFO 139797266573120] Epoch[46] Batch[5] avg_epoch_loss=2.612320\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=2.61232002576\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:02 INFO 139797266573120] Epoch[46] Batch [5]#011Speed: 62.76 samples/sec#011loss=2.612320\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:07 INFO 139797266573120] Epoch[46] Batch[10] avg_epoch_loss=2.646130\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=2.68670229912\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:07 INFO 139797266573120] Epoch[46] Batch [10]#011Speed: 62.46 samples/sec#011loss=2.686702\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:07 INFO 139797266573120] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11919.190883636475, \"sum\": 11919.190883636475, \"min\": 11919.190883636475}}, \"EndTime\": 1580270827.56306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270815.643517}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:07 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.7080437159 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:07 INFO 139797266573120] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=46, train loss <loss>=2.64613015001\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:07 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:09 INFO 139797266573120] Epoch[47] Batch[0] avg_epoch_loss=2.611281\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.61128139496\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:14 INFO 139797266573120] Epoch[47] Batch[5] avg_epoch_loss=2.611998\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.611997962\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:14 INFO 139797266573120] Epoch[47] Batch [5]#011Speed: 62.71 samples/sec#011loss=2.611998\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:18 INFO 139797266573120] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10867.705821990967, \"sum\": 10867.705821990967, \"min\": 10867.705821990967}}, \"EndTime\": 1580270838.431141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270827.563124}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:18 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.7853894314 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:18 INFO 139797266573120] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.60795462132\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:18 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:20 INFO 139797266573120] Epoch[48] Batch[0] avg_epoch_loss=2.560990\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=2.56099009514\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:07:25 INFO 139797266573120] Epoch[48] Batch[5] avg_epoch_loss=2.615657\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.61565677325\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:25 INFO 139797266573120] Epoch[48] Batch [5]#011Speed: 63.05 samples/sec#011loss=2.615657\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:30 INFO 139797266573120] Epoch[48] Batch[10] avg_epoch_loss=2.654871\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:30 INFO 139797266573120] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=2.70192837715\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:30 INFO 139797266573120] Epoch[48] Batch [10]#011Speed: 62.48 samples/sec#011loss=2.701928\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:30 INFO 139797266573120] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11884.73892211914, \"sum\": 11884.73892211914, \"min\": 11884.73892211914}}, \"EndTime\": 1580270850.31636, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270838.431205}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:30 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.1122545912 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:30 INFO 139797266573120] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:30 INFO 139797266573120] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.65487113866\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:30 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:31 INFO 139797266573120] Epoch[49] Batch[0] avg_epoch_loss=2.519559\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:31 INFO 139797266573120] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.51955890656\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:37 INFO 139797266573120] Epoch[49] Batch[5] avg_epoch_loss=2.606407\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.60640736421\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:37 INFO 139797266573120] Epoch[49] Batch [5]#011Speed: 62.95 samples/sec#011loss=2.606407\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:41 INFO 139797266573120] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10845.418930053711, \"sum\": 10845.418930053711, \"min\": 10845.418930053711}}, \"EndTime\": 1580270861.162231, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270850.316421}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:41 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.3648228957 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:41 INFO 139797266573120] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.61179122925\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:41 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:42 INFO 139797266573120] Epoch[50] Batch[0] avg_epoch_loss=2.561263\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:42 INFO 139797266573120] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.56126332283\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:47 INFO 139797266573120] Epoch[50] Batch[5] avg_epoch_loss=2.643024\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:47 INFO 139797266573120] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=2.64302404722\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:47 INFO 139797266573120] Epoch[50] Batch [5]#011Speed: 62.68 samples/sec#011loss=2.643024\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:52 INFO 139797266573120] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10959.27619934082, \"sum\": 10959.27619934082, \"min\": 10959.27619934082}}, \"EndTime\": 1580270872.121977, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270861.162298}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:52 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.9301652934 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:52 INFO 139797266573120] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:52 INFO 139797266573120] #quality_metric: host=algo-1, epoch=50, train loss <loss>=2.61733732224\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:52 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:53 INFO 139797266573120] Epoch[51] Batch[0] avg_epoch_loss=2.653724\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.65372371674\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:58 INFO 139797266573120] Epoch[51] Batch[5] avg_epoch_loss=2.609923\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.60992264748\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:07:58 INFO 139797266573120] Epoch[51] Batch [5]#011Speed: 62.53 samples/sec#011loss=2.609923\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:03 INFO 139797266573120] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10923.699140548706, \"sum\": 10923.699140548706, \"min\": 10923.699140548706}}, \"EndTime\": 1580270883.046175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270872.122041}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:03 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.8553036481 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:03 INFO 139797266573120] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.60629217625\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:03 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:04 INFO 139797266573120] Epoch[52] Batch[0] avg_epoch_loss=2.516788\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=2.51678824425\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:09 INFO 139797266573120] Epoch[52] Batch[5] avg_epoch_loss=2.584918\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.58491834005\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:09 INFO 139797266573120] Epoch[52] Batch [5]#011Speed: 62.30 samples/sec#011loss=2.584918\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:14 INFO 139797266573120] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10961.541175842285, \"sum\": 10961.541175842285, \"min\": 10961.541175842285}}, \"EndTime\": 1580270894.008144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270883.046246}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:14 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.0168067382 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:14 INFO 139797266573120] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.6450514555\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:14 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:15 INFO 139797266573120] Epoch[53] Batch[0] avg_epoch_loss=2.569410\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:15 INFO 139797266573120] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.56940960884\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:20 INFO 139797266573120] Epoch[53] Batch[5] avg_epoch_loss=2.590065\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.59006492297\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:20 INFO 139797266573120] Epoch[53] Batch [5]#011Speed: 62.39 samples/sec#011loss=2.590065\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:26 INFO 139797266573120] Epoch[53] Batch[10] avg_epoch_loss=2.627703\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:26 INFO 139797266573120] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=2.67286849022\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:26 INFO 139797266573120] Epoch[53] Batch [10]#011Speed: 62.27 samples/sec#011loss=2.672868\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:26 INFO 139797266573120] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11999.327898025513, \"sum\": 11999.327898025513, \"min\": 11999.327898025513}}, \"EndTime\": 1580270906.008126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270894.008253}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:26 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.6694004 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:26 INFO 139797266573120] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:26 INFO 139797266573120] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.62770290808\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:26 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:27 INFO 139797266573120] Epoch[54] Batch[0] avg_epoch_loss=2.454025\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.45402479172\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:08:32 INFO 139797266573120] Epoch[54] Batch[5] avg_epoch_loss=2.590002\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.59000174205\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:32 INFO 139797266573120] Epoch[54] Batch [5]#011Speed: 62.81 samples/sec#011loss=2.590002\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:37 INFO 139797266573120] Epoch[54] Batch[10] avg_epoch_loss=2.615038\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=2.64508194923\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:37 INFO 139797266573120] Epoch[54] Batch [10]#011Speed: 62.31 samples/sec#011loss=2.645082\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:37 INFO 139797266573120] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11916.575908660889, \"sum\": 11916.575908660889, \"min\": 11916.575908660889}}, \"EndTime\": 1580270917.92505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270906.008189}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:37 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.9720264789 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:37 INFO 139797266573120] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.61503819986\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:37 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:39 INFO 139797266573120] Epoch[55] Batch[0] avg_epoch_loss=2.579093\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.57909345627\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:44 INFO 139797266573120] Epoch[55] Batch[5] avg_epoch_loss=2.615072\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.61507209142\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:44 INFO 139797266573120] Epoch[55] Batch [5]#011Speed: 62.60 samples/sec#011loss=2.615072\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:49 INFO 139797266573120] Epoch[55] Batch[10] avg_epoch_loss=2.630851\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=2.64978480339\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:49 INFO 139797266573120] Epoch[55] Batch [10]#011Speed: 61.97 samples/sec#011loss=2.649785\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:49 INFO 139797266573120] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12017.622947692871, \"sum\": 12017.622947692871, \"min\": 12017.622947692871}}, \"EndTime\": 1580270929.943034, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270917.925113}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:49 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.8342049197 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:49 INFO 139797266573120] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.63085059686\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:49 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:51 INFO 139797266573120] Epoch[56] Batch[0] avg_epoch_loss=2.577729\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.57772874832\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:56 INFO 139797266573120] Epoch[56] Batch[5] avg_epoch_loss=2.566517\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.56651675701\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:08:56 INFO 139797266573120] Epoch[56] Batch [5]#011Speed: 62.92 samples/sec#011loss=2.566517\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:00 INFO 139797266573120] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10873.719930648804, \"sum\": 10873.719930648804, \"min\": 10873.719930648804}}, \"EndTime\": 1580270940.817211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270929.943102}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:00 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.5809826107 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:00 INFO 139797266573120] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.58340549469\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:00 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:02 INFO 139797266573120] Epoch[57] Batch[0] avg_epoch_loss=2.589686\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=2.5896859169\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:07 INFO 139797266573120] Epoch[57] Batch[5] avg_epoch_loss=2.578297\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=2.57829729716\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:07 INFO 139797266573120] Epoch[57] Batch [5]#011Speed: 62.13 samples/sec#011loss=2.578297\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:12 INFO 139797266573120] Epoch[57] Batch[10] avg_epoch_loss=2.579811\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=2.58162665367\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:12 INFO 139797266573120] Epoch[57] Batch [10]#011Speed: 62.06 samples/sec#011loss=2.581627\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:12 INFO 139797266573120] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12018.126010894775, \"sum\": 12018.126010894775, \"min\": 12018.126010894775}}, \"EndTime\": 1580270952.835877, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270940.817295}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:12 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.5807719306 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:12 INFO 139797266573120] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=57, train loss <loss>=2.57981064103\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:12 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:14 INFO 139797266573120] Epoch[58] Batch[0] avg_epoch_loss=2.646363\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.64636325836\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:19 INFO 139797266573120] Epoch[58] Batch[5] avg_epoch_loss=2.591568\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:19 INFO 139797266573120] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=2.59156787395\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:19 INFO 139797266573120] Epoch[58] Batch [5]#011Speed: 62.68 samples/sec#011loss=2.591568\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:24 INFO 139797266573120] Epoch[58] Batch[10] avg_epoch_loss=2.623459\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=2.66172766685\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:24 INFO 139797266573120] Epoch[58] Batch [10]#011Speed: 62.04 samples/sec#011loss=2.661728\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:24 INFO 139797266573120] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11977.828025817871, \"sum\": 11977.828025817871, \"min\": 11977.828025817871}}, \"EndTime\": 1580270964.814058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270952.835939}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:24 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.3536624877 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:24 INFO 139797266573120] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=58, train loss <loss>=2.62345868891\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:24 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:26 INFO 139797266573120] Epoch[59] Batch[0] avg_epoch_loss=2.665578\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:26 INFO 139797266573120] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=2.66557788849\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:31 INFO 139797266573120] Epoch[59] Batch[5] avg_epoch_loss=2.575578\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:31 INFO 139797266573120] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=2.57557821274\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:31 INFO 139797266573120] Epoch[59] Batch [5]#011Speed: 62.68 samples/sec#011loss=2.575578\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] Epoch[59] Batch[10] avg_epoch_loss=2.537834\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=2.49253993034\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] Epoch[59] Batch [10]#011Speed: 61.89 samples/sec#011loss=2.492540\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11964.980125427246, \"sum\": 11964.980125427246, \"min\": 11964.980125427246}}, \"EndTime\": 1580270976.779627, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270964.814123}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.7455905531 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=59, train loss <loss>=2.53783353892\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:36 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_242104ea-fc47-4b26-9da8-1517b8c382b1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 205.42287826538086, \"sum\": 205.42287826538086, \"min\": 205.42287826538086}}, \"EndTime\": 1580270976.985491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270976.779692}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:38 INFO 139797266573120] Epoch[60] Batch[0] avg_epoch_loss=2.551270\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=2.55126953125\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:43 INFO 139797266573120] Epoch[60] Batch[5] avg_epoch_loss=2.530607\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.53060674667\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:43 INFO 139797266573120] Epoch[60] Batch [5]#011Speed: 62.73 samples/sec#011loss=2.530607\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:48 INFO 139797266573120] Epoch[60] Batch[10] avg_epoch_loss=2.517644\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=2.50208873749\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:48 INFO 139797266573120] Epoch[60] Batch [10]#011Speed: 62.47 samples/sec#011loss=2.502089\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:48 INFO 139797266573120] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11935.76693534851, \"sum\": 11935.76693534851, \"min\": 11935.76693534851}}, \"EndTime\": 1580270988.921366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270976.985548}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:48 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.4685046703 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:48 INFO 139797266573120] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.51764401523\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:48 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:49 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_d55e5fda-9924-48f6-894b-b6c90b67f598-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 216.86601638793945, \"sum\": 216.86601638793945, \"min\": 216.86601638793945}}, \"EndTime\": 1580270989.138614, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270988.921428}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:50 INFO 139797266573120] Epoch[61] Batch[0] avg_epoch_loss=2.592402\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:50 INFO 139797266573120] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.59240198135\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:55 INFO 139797266573120] Epoch[61] Batch[5] avg_epoch_loss=2.534746\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.534745574\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:09:55 INFO 139797266573120] Epoch[61] Batch [5]#011Speed: 62.78 samples/sec#011loss=2.534746\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:00 INFO 139797266573120] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10932.384967803955, \"sum\": 10932.384967803955, \"min\": 10932.384967803955}}, \"EndTime\": 1580271000.071122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580270989.138678}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:00 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.7117539844 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:00 INFO 139797266573120] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.56918959618\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:00 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:01 INFO 139797266573120] Epoch[62] Batch[0] avg_epoch_loss=2.535378\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:01 INFO 139797266573120] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.53537774086\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:06 INFO 139797266573120] Epoch[62] Batch[5] avg_epoch_loss=2.534110\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.53411022822\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:06 INFO 139797266573120] Epoch[62] Batch [5]#011Speed: 62.71 samples/sec#011loss=2.534110\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:10 INFO 139797266573120] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10922.875881195068, \"sum\": 10922.875881195068, \"min\": 10922.875881195068}}, \"EndTime\": 1580271010.994498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271000.071182}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:10 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.7611137501 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:10 INFO 139797266573120] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.53399038315\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:10 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:12 INFO 139797266573120] Epoch[63] Batch[0] avg_epoch_loss=2.581032\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.58103227615\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:17 INFO 139797266573120] Epoch[63] Batch[5] avg_epoch_loss=2.544326\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.54432622592\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:17 INFO 139797266573120] Epoch[63] Batch [5]#011Speed: 62.07 samples/sec#011loss=2.544326\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:22 INFO 139797266573120] Epoch[63] Batch[10] avg_epoch_loss=2.623463\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=2.71842684746\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:22 INFO 139797266573120] Epoch[63] Batch [10]#011Speed: 61.87 samples/sec#011loss=2.718427\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:22 INFO 139797266573120] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11994.163990020752, \"sum\": 11994.163990020752, \"min\": 11994.163990020752}}, \"EndTime\": 1580271022.989129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271010.994564}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:22 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.5256112674 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:22 INFO 139797266573120] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.62346287207\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:22 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:24 INFO 139797266573120] Epoch[64] Batch[0] avg_epoch_loss=2.449430\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.4494304657\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:29 INFO 139797266573120] Epoch[64] Batch[5] avg_epoch_loss=2.581438\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.58143762747\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:29 INFO 139797266573120] Epoch[64] Batch [5]#011Speed: 61.85 samples/sec#011loss=2.581438\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:10:35 INFO 139797266573120] Epoch[64] Batch[10] avg_epoch_loss=2.636081\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:35 INFO 139797266573120] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=2.70165305138\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:35 INFO 139797266573120] Epoch[64] Batch [10]#011Speed: 61.86 samples/sec#011loss=2.701653\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:35 INFO 139797266573120] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12034.975051879883, \"sum\": 12034.975051879883, \"min\": 12034.975051879883}}, \"EndTime\": 1580271035.024559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271022.989191}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:35 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.9228347068 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:35 INFO 139797266573120] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:35 INFO 139797266573120] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.63608100198\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:35 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:36 INFO 139797266573120] Epoch[65] Batch[0] avg_epoch_loss=2.727314\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=2.7273144722\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:41 INFO 139797266573120] Epoch[65] Batch[5] avg_epoch_loss=2.606887\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.60688734055\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:41 INFO 139797266573120] Epoch[65] Batch [5]#011Speed: 62.63 samples/sec#011loss=2.606887\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:45 INFO 139797266573120] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10866.240978240967, \"sum\": 10866.240978240967, \"min\": 10866.240978240967}}, \"EndTime\": 1580271045.891221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271035.024622}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:45 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.204063607 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:45 INFO 139797266573120] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:45 INFO 139797266573120] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.49927011728\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:45 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:46 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_cdde259b-ebe1-44d0-b9cd-3bcfc07df2ad-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 211.40718460083008, \"sum\": 211.40718460083008, \"min\": 211.40718460083008}}, \"EndTime\": 1580271046.103262, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271045.89129}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:47 INFO 139797266573120] Epoch[66] Batch[0] avg_epoch_loss=2.691292\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:47 INFO 139797266573120] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=2.6912920475\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:52 INFO 139797266573120] Epoch[66] Batch[5] avg_epoch_loss=2.565574\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:52 INFO 139797266573120] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=2.5655742089\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:52 INFO 139797266573120] Epoch[66] Batch [5]#011Speed: 62.86 samples/sec#011loss=2.565574\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:56 INFO 139797266573120] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10864.781141281128, \"sum\": 10864.781141281128, \"min\": 10864.781141281128}}, \"EndTime\": 1580271056.968159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271046.103322}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:56 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.6040964825 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:56 INFO 139797266573120] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=66, train loss <loss>=2.60274648666\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:56 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:58 INFO 139797266573120] Epoch[67] Batch[0] avg_epoch_loss=2.602053\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:10:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=2.60205316544\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:03 INFO 139797266573120] Epoch[67] Batch[5] avg_epoch_loss=2.562026\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=2.56202618281\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:03 INFO 139797266573120] Epoch[67] Batch [5]#011Speed: 62.62 samples/sec#011loss=2.562026\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:08 INFO 139797266573120] Epoch[67] Batch[10] avg_epoch_loss=2.554684\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:08 INFO 139797266573120] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=2.54587416649\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:08 INFO 139797266573120] Epoch[67] Batch [10]#011Speed: 62.20 samples/sec#011loss=2.545874\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:08 INFO 139797266573120] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11968.890190124512, \"sum\": 11968.890190124512, \"min\": 11968.890190124512}}, \"EndTime\": 1580271068.937506, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271056.968282}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:08 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.9806415259 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:08 INFO 139797266573120] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:08 INFO 139797266573120] #quality_metric: host=algo-1, epoch=67, train loss <loss>=2.55468435721\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:08 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:10 INFO 139797266573120] Epoch[68] Batch[0] avg_epoch_loss=2.561621\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=2.5616209507\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:15 INFO 139797266573120] Epoch[68] Batch[5] avg_epoch_loss=2.550946\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:15 INFO 139797266573120] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=2.55094603697\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:15 INFO 139797266573120] Epoch[68] Batch [5]#011Speed: 62.42 samples/sec#011loss=2.550946\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:20 INFO 139797266573120] Epoch[68] Batch[10] avg_epoch_loss=2.653666\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=2.77692894936\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:20 INFO 139797266573120] Epoch[68] Batch [10]#011Speed: 61.61 samples/sec#011loss=2.776929\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:20 INFO 139797266573120] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12024.924993515015, \"sum\": 12024.924993515015, \"min\": 12024.924993515015}}, \"EndTime\": 1580271080.962961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271068.937566}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:20 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.3865904368 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:20 INFO 139797266573120] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=68, train loss <loss>=2.6536655426\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:20 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:22 INFO 139797266573120] Epoch[69] Batch[0] avg_epoch_loss=2.484185\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.48418545723\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:27 INFO 139797266573120] Epoch[69] Batch[5] avg_epoch_loss=2.568714\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.56871394316\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:27 INFO 139797266573120] Epoch[69] Batch [5]#011Speed: 62.32 samples/sec#011loss=2.568714\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:32 INFO 139797266573120] Epoch[69] Batch[10] avg_epoch_loss=2.564360\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=2.55913486481\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:32 INFO 139797266573120] Epoch[69] Batch [10]#011Speed: 62.11 samples/sec#011loss=2.559135\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:32 INFO 139797266573120] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11971.17304801941, \"sum\": 11971.17304801941, \"min\": 11971.17304801941}}, \"EndTime\": 1580271092.934562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271080.963024}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:32 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.9673570317 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:32 INFO 139797266573120] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.56435981664\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:32 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:34 INFO 139797266573120] Epoch[70] Batch[0] avg_epoch_loss=2.576988\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=2.57698750496\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:11:39 INFO 139797266573120] Epoch[70] Batch[5] avg_epoch_loss=2.554444\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=2.55444435279\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:39 INFO 139797266573120] Epoch[70] Batch [5]#011Speed: 62.23 samples/sec#011loss=2.554444\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:44 INFO 139797266573120] Epoch[70] Batch[10] avg_epoch_loss=2.520675\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=2.48015136719\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:44 INFO 139797266573120] Epoch[70] Batch [10]#011Speed: 62.09 samples/sec#011loss=2.480151\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:44 INFO 139797266573120] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11963.309049606323, \"sum\": 11963.309049606323, \"min\": 11963.309049606323}}, \"EndTime\": 1580271104.898417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271092.934619}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:44 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.5831686184 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:44 INFO 139797266573120] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=70, train loss <loss>=2.52067481388\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:44 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:46 INFO 139797266573120] Epoch[71] Batch[0] avg_epoch_loss=2.616383\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=2.61638331413\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:51 INFO 139797266573120] Epoch[71] Batch[5] avg_epoch_loss=2.682883\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=2.68288250764\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:51 INFO 139797266573120] Epoch[71] Batch [5]#011Speed: 62.94 samples/sec#011loss=2.682883\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:56 INFO 139797266573120] Epoch[71] Batch[10] avg_epoch_loss=2.589027\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=2.47639956474\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:56 INFO 139797266573120] Epoch[71] Batch [10]#011Speed: 62.63 samples/sec#011loss=2.476400\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:56 INFO 139797266573120] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11876.310110092163, \"sum\": 11876.310110092163, \"min\": 11876.310110092163}}, \"EndTime\": 1580271116.775136, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271104.898475}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:56 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.2355700199 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:56 INFO 139797266573120] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=71, train loss <loss>=2.58902662451\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:56 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:58 INFO 139797266573120] Epoch[72] Batch[0] avg_epoch_loss=2.663632\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:11:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=2.66363239288\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:03 INFO 139797266573120] Epoch[72] Batch[5] avg_epoch_loss=2.586992\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=2.58699182669\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:03 INFO 139797266573120] Epoch[72] Batch [5]#011Speed: 62.86 samples/sec#011loss=2.586992\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:07 INFO 139797266573120] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10874.393939971924, \"sum\": 10874.393939971924, \"min\": 10874.393939971924}}, \"EndTime\": 1580271127.649955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271116.775201}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:07 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.1865441159 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:07 INFO 139797266573120] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=72, train loss <loss>=2.54216825962\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:07 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:09 INFO 139797266573120] Epoch[73] Batch[0] avg_epoch_loss=2.442301\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.44230103493\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:14 INFO 139797266573120] Epoch[73] Batch[5] avg_epoch_loss=2.548541\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=2.54854094982\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:14 INFO 139797266573120] Epoch[73] Batch [5]#011Speed: 62.86 samples/sec#011loss=2.548541\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:19 INFO 139797266573120] Epoch[73] Batch[10] avg_epoch_loss=2.520725\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:19 INFO 139797266573120] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=2.48734560013\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:19 INFO 139797266573120] Epoch[73] Batch [10]#011Speed: 62.36 samples/sec#011loss=2.487346\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:19 INFO 139797266573120] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11888.84711265564, \"sum\": 11888.84711265564, \"min\": 11888.84711265564}}, \"EndTime\": 1580271139.539395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271127.650021}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:19 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.168005112 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:19 INFO 139797266573120] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:19 INFO 139797266573120] #quality_metric: host=algo-1, epoch=73, train loss <loss>=2.52072488178\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:19 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:21 INFO 139797266573120] Epoch[74] Batch[0] avg_epoch_loss=2.599388\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=2.59938788414\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:26 INFO 139797266573120] Epoch[74] Batch[5] avg_epoch_loss=2.555749\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:26 INFO 139797266573120] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=2.55574874083\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:26 INFO 139797266573120] Epoch[74] Batch [5]#011Speed: 63.15 samples/sec#011loss=2.555749\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:31 INFO 139797266573120] Epoch[74] Batch[10] avg_epoch_loss=2.554288\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:31 INFO 139797266573120] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=2.55253601074\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:31 INFO 139797266573120] Epoch[74] Batch [10]#011Speed: 62.60 samples/sec#011loss=2.552536\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:31 INFO 139797266573120] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11871.068954467773, \"sum\": 11871.068954467773, \"min\": 11871.068954467773}}, \"EndTime\": 1580271151.41079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271139.539456}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:31 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.0181008452 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:31 INFO 139797266573120] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:31 INFO 139797266573120] #quality_metric: host=algo-1, epoch=74, train loss <loss>=2.55428840897\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:31 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:33 INFO 139797266573120] Epoch[75] Batch[0] avg_epoch_loss=2.680773\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:33 INFO 139797266573120] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=2.68077301979\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:12:38 INFO 139797266573120] Epoch[75] Batch[5] avg_epoch_loss=2.642168\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=2.64216824373\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:38 INFO 139797266573120] Epoch[75] Batch [5]#011Speed: 62.60 samples/sec#011loss=2.642168\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:43 INFO 139797266573120] Epoch[75] Batch[10] avg_epoch_loss=2.555568\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=2.45164835453\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:43 INFO 139797266573120] Epoch[75] Batch [10]#011Speed: 62.77 samples/sec#011loss=2.451648\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:43 INFO 139797266573120] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11881.94489479065, \"sum\": 11881.94489479065, \"min\": 11881.94489479065}}, \"EndTime\": 1580271163.293231, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271151.410852}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:43 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.4617820451 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:43 INFO 139797266573120] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=75, train loss <loss>=2.55556829409\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:43 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:44 INFO 139797266573120] Epoch[76] Batch[0] avg_epoch_loss=2.602744\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=2.60274410248\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:50 INFO 139797266573120] Epoch[76] Batch[5] avg_epoch_loss=2.580099\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:50 INFO 139797266573120] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=2.58009914557\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:50 INFO 139797266573120] Epoch[76] Batch [5]#011Speed: 62.53 samples/sec#011loss=2.580099\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:55 INFO 139797266573120] Epoch[76] Batch[10] avg_epoch_loss=2.601731\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=2.6276892662\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:55 INFO 139797266573120] Epoch[76] Batch [10]#011Speed: 62.49 samples/sec#011loss=2.627689\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:55 INFO 139797266573120] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11902.683973312378, \"sum\": 11902.683973312378, \"min\": 11902.683973312378}}, \"EndTime\": 1580271175.19626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271163.293313}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:55 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.1890623798 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:55 INFO 139797266573120] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=76, train loss <loss>=2.60173101859\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:55 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:56 INFO 139797266573120] Epoch[77] Batch[0] avg_epoch_loss=2.521840\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:12:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.52184009552\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:01 INFO 139797266573120] Epoch[77] Batch[5] avg_epoch_loss=2.601546\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:01 INFO 139797266573120] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.60154608885\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:01 INFO 139797266573120] Epoch[77] Batch [5]#011Speed: 62.69 samples/sec#011loss=2.601546\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:06 INFO 139797266573120] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10846.50206565857, \"sum\": 10846.50206565857, \"min\": 10846.50206565857}}, \"EndTime\": 1580271186.043107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271175.196319}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:06 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.3448850297 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:06 INFO 139797266573120] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=77, train loss <loss>=2.57958974838\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:06 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:07 INFO 139797266573120] Epoch[78] Batch[0] avg_epoch_loss=2.549173\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=2.54917263985\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:12 INFO 139797266573120] Epoch[78] Batch[5] avg_epoch_loss=2.561639\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=2.56163891157\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:12 INFO 139797266573120] Epoch[78] Batch [5]#011Speed: 62.25 samples/sec#011loss=2.561639\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:17 INFO 139797266573120] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10994.876861572266, \"sum\": 10994.876861572266, \"min\": 10994.876861572266}}, \"EndTime\": 1580271197.038581, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271186.043221}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:17 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.662134479 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:17 INFO 139797266573120] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=78, train loss <loss>=2.57831835747\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:17 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:18 INFO 139797266573120] Epoch[79] Batch[0] avg_epoch_loss=2.545232\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.54523229599\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:23 INFO 139797266573120] Epoch[79] Batch[5] avg_epoch_loss=2.580857\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=2.58085723718\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:23 INFO 139797266573120] Epoch[79] Batch [5]#011Speed: 62.43 samples/sec#011loss=2.580857\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:27 INFO 139797266573120] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10886.980056762695, \"sum\": 10886.980056762695, \"min\": 10886.980056762695}}, \"EndTime\": 1580271207.926265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271197.038668}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:27 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.866734525 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:27 INFO 139797266573120] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=79, train loss <loss>=2.57574007511\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:27 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:29 INFO 139797266573120] Epoch[80] Batch[0] avg_epoch_loss=2.597890\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=2.59788966179\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:34 INFO 139797266573120] Epoch[80] Batch[5] avg_epoch_loss=2.564564\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=2.56456371148\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:34 INFO 139797266573120] Epoch[80] Batch [5]#011Speed: 62.89 samples/sec#011loss=2.564564\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:38 INFO 139797266573120] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10852.417945861816, \"sum\": 10852.417945861816, \"min\": 10852.417945861816}}, \"EndTime\": 1580271218.779241, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271207.926338}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:38 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.4982093307 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:38 INFO 139797266573120] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=80, train loss <loss>=2.54915332794\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:38 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:40 INFO 139797266573120] Epoch[81] Batch[0] avg_epoch_loss=2.608453\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:40 INFO 139797266573120] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=2.60845327377\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:13:45 INFO 139797266573120] Epoch[81] Batch[5] avg_epoch_loss=2.535136\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:45 INFO 139797266573120] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=2.53513646126\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:45 INFO 139797266573120] Epoch[81] Batch [5]#011Speed: 62.87 samples/sec#011loss=2.535136\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:49 INFO 139797266573120] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10827.65793800354, \"sum\": 10827.65793800354, \"min\": 10827.65793800354}}, \"EndTime\": 1580271229.607358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271218.779308}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:49 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.7061424802 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:49 INFO 139797266573120] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=81, train loss <loss>=2.5616414547\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:49 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:51 INFO 139797266573120] Epoch[82] Batch[0] avg_epoch_loss=2.625110\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=2.62510967255\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:56 INFO 139797266573120] Epoch[82] Batch[5] avg_epoch_loss=2.563739\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=2.56373862425\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:13:56 INFO 139797266573120] Epoch[82] Batch [5]#011Speed: 62.83 samples/sec#011loss=2.563739\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:01 INFO 139797266573120] Epoch[82] Batch[10] avg_epoch_loss=2.573558\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:01 INFO 139797266573120] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=2.5853407383\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:01 INFO 139797266573120] Epoch[82] Batch [10]#011Speed: 61.56 samples/sec#011loss=2.585341\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:01 INFO 139797266573120] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11980.73410987854, \"sum\": 11980.73410987854, \"min\": 11980.73410987854}}, \"EndTime\": 1580271241.588612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271229.607417}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:01 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.3383933996 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:01 INFO 139797266573120] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:01 INFO 139797266573120] #quality_metric: host=algo-1, epoch=82, train loss <loss>=2.573557767\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:01 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:03 INFO 139797266573120] Epoch[83] Batch[0] avg_epoch_loss=2.448608\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=2.44860768318\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:08 INFO 139797266573120] Epoch[83] Batch[5] avg_epoch_loss=2.538351\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:08 INFO 139797266573120] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=2.53835121791\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:08 INFO 139797266573120] Epoch[83] Batch [5]#011Speed: 62.22 samples/sec#011loss=2.538351\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:12 INFO 139797266573120] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11109.328031539917, \"sum\": 11109.328031539917, \"min\": 11109.328031539917}}, \"EndTime\": 1580271252.698385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271241.588675}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:12 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.9983114337 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:12 INFO 139797266573120] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=83, train loss <loss>=2.5402793169\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:12 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:14 INFO 139797266573120] Epoch[84] Batch[0] avg_epoch_loss=2.409873\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=2.40987277031\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:19 INFO 139797266573120] Epoch[84] Batch[5] avg_epoch_loss=2.539161\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:19 INFO 139797266573120] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=2.53916072845\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:19 INFO 139797266573120] Epoch[84] Batch [5]#011Speed: 61.98 samples/sec#011loss=2.539161\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:23 INFO 139797266573120] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10941.475868225098, \"sum\": 10941.475868225098, \"min\": 10941.475868225098}}, \"EndTime\": 1580271263.640352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271252.698456}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:23 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.7496282832 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:23 INFO 139797266573120] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=84, train loss <loss>=2.51841857433\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:23 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:25 INFO 139797266573120] Epoch[85] Batch[0] avg_epoch_loss=2.485422\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=2.48542237282\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:30 INFO 139797266573120] Epoch[85] Batch[5] avg_epoch_loss=2.525740\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:30 INFO 139797266573120] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=2.5257401069\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:30 INFO 139797266573120] Epoch[85] Batch [5]#011Speed: 62.78 samples/sec#011loss=2.525740\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:34 INFO 139797266573120] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10887.793064117432, \"sum\": 10887.793064117432, \"min\": 10887.793064117432}}, \"EndTime\": 1580271274.528787, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271263.640618}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:34 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.4950709394 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:34 INFO 139797266573120] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=85, train loss <loss>=2.56905441284\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:34 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:36 INFO 139797266573120] Epoch[86] Batch[0] avg_epoch_loss=2.545604\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=2.54560399055\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:41 INFO 139797266573120] Epoch[86] Batch[5] avg_epoch_loss=2.533385\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=2.53338460128\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:41 INFO 139797266573120] Epoch[86] Batch [5]#011Speed: 62.31 samples/sec#011loss=2.533385\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:46 INFO 139797266573120] Epoch[86] Batch[10] avg_epoch_loss=2.515726\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=2.49453639984\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:46 INFO 139797266573120] Epoch[86] Batch [10]#011Speed: 61.80 samples/sec#011loss=2.494536\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:46 INFO 139797266573120] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12027.887105941772, \"sum\": 12027.887105941772, \"min\": 12027.887105941772}}, \"EndTime\": 1580271286.557179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271274.528853}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:46 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.454028205 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:46 INFO 139797266573120] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=86, train loss <loss>=2.5157263279\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:46 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:48 INFO 139797266573120] Epoch[87] Batch[0] avg_epoch_loss=2.591144\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=2.59114384651\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:53 INFO 139797266573120] Epoch[87] Batch[5] avg_epoch_loss=2.582515\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=2.58251484235\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:53 INFO 139797266573120] Epoch[87] Batch [5]#011Speed: 62.51 samples/sec#011loss=2.582515\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:14:58 INFO 139797266573120] Epoch[87] Batch[10] avg_epoch_loss=2.556320\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=2.52488660812\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:58 INFO 139797266573120] Epoch[87] Batch [10]#011Speed: 62.33 samples/sec#011loss=2.524887\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:58 INFO 139797266573120] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11973.366975784302, \"sum\": 11973.366975784302, \"min\": 11973.366975784302}}, \"EndTime\": 1580271298.53101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271286.55724}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:58 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.1263252895 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:58 INFO 139797266573120] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=87, train loss <loss>=2.55632019043\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:14:58 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:00 INFO 139797266573120] Epoch[88] Batch[0] avg_epoch_loss=2.529066\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=2.5290658474\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:05 INFO 139797266573120] Epoch[88] Batch[5] avg_epoch_loss=2.601488\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:05 INFO 139797266573120] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=2.6014876763\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:05 INFO 139797266573120] Epoch[88] Batch [5]#011Speed: 62.08 samples/sec#011loss=2.601488\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:09 INFO 139797266573120] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10966.624975204468, \"sum\": 10966.624975204468, \"min\": 10966.624975204468}}, \"EndTime\": 1580271309.498241, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271298.531074}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:09 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.1637614802 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:09 INFO 139797266573120] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=88, train loss <loss>=2.60487766266\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:09 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:11 INFO 139797266573120] Epoch[89] Batch[0] avg_epoch_loss=2.543020\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=2.54302000999\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:16 INFO 139797266573120] Epoch[89] Batch[5] avg_epoch_loss=2.532687\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:16 INFO 139797266573120] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=2.53268718719\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:16 INFO 139797266573120] Epoch[89] Batch [5]#011Speed: 62.29 samples/sec#011loss=2.532687\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:20 INFO 139797266573120] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10948.591947555542, \"sum\": 10948.591947555542, \"min\": 10948.591947555542}}, \"EndTime\": 1580271320.447356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271309.498306}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:20 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.2718169214 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:20 INFO 139797266573120] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=89, train loss <loss>=2.54979083538\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:20 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:22 INFO 139797266573120] Epoch[90] Batch[0] avg_epoch_loss=2.628186\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=2.62818622589\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:27 INFO 139797266573120] Epoch[90] Batch[5] avg_epoch_loss=2.576248\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=2.57624816895\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:27 INFO 139797266573120] Epoch[90] Batch [5]#011Speed: 62.88 samples/sec#011loss=2.576248\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] Epoch[90] Batch[10] avg_epoch_loss=2.463317\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=2.3277995348\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] Epoch[90] Batch [10]#011Speed: 62.17 samples/sec#011loss=2.327800\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11928.071022033691, \"sum\": 11928.071022033691, \"min\": 11928.071022033691}}, \"EndTime\": 1580271332.375846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271320.44742}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.2410732723 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=90, train loss <loss>=2.46331697161\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:32 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_e684f4df-a056-47ec-9850-a68c5600b288-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 227.20885276794434, \"sum\": 227.20885276794434, \"min\": 227.20885276794434}}, \"EndTime\": 1580271332.603789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271332.375942}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:34 INFO 139797266573120] Epoch[91] Batch[0] avg_epoch_loss=2.728487\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=2.72848749161\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:39 INFO 139797266573120] Epoch[91] Batch[5] avg_epoch_loss=2.578196\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=2.57819612821\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:39 INFO 139797266573120] Epoch[91] Batch [5]#011Speed: 62.58 samples/sec#011loss=2.578196\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:44 INFO 139797266573120] Epoch[91] Batch[10] avg_epoch_loss=2.565766\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=2.55084896088\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:44 INFO 139797266573120] Epoch[91] Batch [10]#011Speed: 62.10 samples/sec#011loss=2.550849\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:44 INFO 139797266573120] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11972.91088104248, \"sum\": 11972.91088104248, \"min\": 11972.91088104248}}, \"EndTime\": 1580271344.576812, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271332.603846}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:44 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.62514308 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:44 INFO 139797266573120] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=91, train loss <loss>=2.5657655976\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:44 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:46 INFO 139797266573120] Epoch[92] Batch[0] avg_epoch_loss=2.583744\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=2.58374404907\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:51 INFO 139797266573120] Epoch[92] Batch[5] avg_epoch_loss=2.557582\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=2.5575816234\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:51 INFO 139797266573120] Epoch[92] Batch [5]#011Speed: 62.11 samples/sec#011loss=2.557582\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:15:56 INFO 139797266573120] Epoch[92] Batch[10] avg_epoch_loss=2.553357\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=2.54828767776\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:56 INFO 139797266573120] Epoch[92] Batch [10]#011Speed: 62.44 samples/sec#011loss=2.548288\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:56 INFO 139797266573120] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12023.453950881958, \"sum\": 12023.453950881958, \"min\": 12023.453950881958}}, \"EndTime\": 1580271356.600609, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271344.576876}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:56 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.3061522609 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:56 INFO 139797266573120] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=92, train loss <loss>=2.55335710265\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:56 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:58 INFO 139797266573120] Epoch[93] Batch[0] avg_epoch_loss=2.522707\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:15:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=2.52270650864\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:03 INFO 139797266573120] Epoch[93] Batch[5] avg_epoch_loss=2.521398\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=2.52139778932\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:03 INFO 139797266573120] Epoch[93] Batch [5]#011Speed: 62.04 samples/sec#011loss=2.521398\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:07 INFO 139797266573120] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11010.280132293701, \"sum\": 11010.280132293701, \"min\": 11010.280132293701}}, \"EndTime\": 1580271367.611358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271356.600679}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:07 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.0369211677 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:07 INFO 139797266573120] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=93, train loss <loss>=2.53373167515\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:07 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:09 INFO 139797266573120] Epoch[94] Batch[0] avg_epoch_loss=2.554472\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=2.55447173119\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:14 INFO 139797266573120] Epoch[94] Batch[5] avg_epoch_loss=2.561783\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=2.56178283691\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:14 INFO 139797266573120] Epoch[94] Batch [5]#011Speed: 61.42 samples/sec#011loss=2.561783\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:18 INFO 139797266573120] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11058.301210403442, \"sum\": 11058.301210403442, \"min\": 11058.301210403442}}, \"EndTime\": 1580271378.670108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271367.611447}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:18 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.4382836656 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:18 INFO 139797266573120] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=94, train loss <loss>=2.48045494556\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:18 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:20 INFO 139797266573120] Epoch[95] Batch[0] avg_epoch_loss=2.530544\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=2.53054356575\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:25 INFO 139797266573120] Epoch[95] Batch[5] avg_epoch_loss=2.549892\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=2.54989246527\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:25 INFO 139797266573120] Epoch[95] Batch [5]#011Speed: 62.02 samples/sec#011loss=2.549892\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:30 INFO 139797266573120] Epoch[95] Batch[10] avg_epoch_loss=2.544829\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:30 INFO 139797266573120] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=2.53875350952\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:30 INFO 139797266573120] Epoch[95] Batch [10]#011Speed: 61.73 samples/sec#011loss=2.538754\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:30 INFO 139797266573120] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12081.195831298828, \"sum\": 12081.195831298828, \"min\": 12081.195831298828}}, \"EndTime\": 1580271390.751813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271378.67017}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:30 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.2921043119 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:30 INFO 139797266573120] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:30 INFO 139797266573120] #quality_metric: host=algo-1, epoch=95, train loss <loss>=2.54482930357\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:30 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:32 INFO 139797266573120] Epoch[96] Batch[0] avg_epoch_loss=2.517861\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=2.51786065102\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:37 INFO 139797266573120] Epoch[96] Batch[5] avg_epoch_loss=2.501581\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=2.50158075492\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:37 INFO 139797266573120] Epoch[96] Batch [5]#011Speed: 62.38 samples/sec#011loss=2.501581\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:42 INFO 139797266573120] Epoch[96] Batch[10] avg_epoch_loss=2.544508\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:42 INFO 139797266573120] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=2.59602022171\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:42 INFO 139797266573120] Epoch[96] Batch [10]#011Speed: 62.33 samples/sec#011loss=2.596020\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:42 INFO 139797266573120] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12010.292053222656, \"sum\": 12010.292053222656, \"min\": 12010.292053222656}}, \"EndTime\": 1580271402.76249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271390.751877}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:42 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.0357043503 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:42 INFO 139797266573120] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:42 INFO 139797266573120] #quality_metric: host=algo-1, epoch=96, train loss <loss>=2.54450778528\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:42 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:44 INFO 139797266573120] Epoch[97] Batch[0] avg_epoch_loss=2.503008\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=2.50300788879\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:49 INFO 139797266573120] Epoch[97] Batch[5] avg_epoch_loss=2.568565\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=2.56856497129\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:49 INFO 139797266573120] Epoch[97] Batch [5]#011Speed: 62.55 samples/sec#011loss=2.568565\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:53 INFO 139797266573120] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10925.33802986145, \"sum\": 10925.33802986145, \"min\": 10925.33802986145}}, \"EndTime\": 1580271413.688236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271402.762552}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:53 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.1211968879 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:53 INFO 139797266573120] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=97, train loss <loss>=2.55673627853\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:53 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:55 INFO 139797266573120] Epoch[98] Batch[0] avg_epoch_loss=2.620288\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:16:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=2.62028765678\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:17:00 INFO 139797266573120] Epoch[98] Batch[5] avg_epoch_loss=2.556734\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=2.55673404535\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:00 INFO 139797266573120] Epoch[98] Batch [5]#011Speed: 62.76 samples/sec#011loss=2.556734\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:04 INFO 139797266573120] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10913.086891174316, \"sum\": 10913.086891174316, \"min\": 10913.086891174316}}, \"EndTime\": 1580271424.601851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271413.688308}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:04 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.8957157327 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:04 INFO 139797266573120] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=98, train loss <loss>=2.54458897114\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:04 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:06 INFO 139797266573120] Epoch[99] Batch[0] avg_epoch_loss=2.555221\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=2.5552213192\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:11 INFO 139797266573120] Epoch[99] Batch[5] avg_epoch_loss=2.553138\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=2.5531377395\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:11 INFO 139797266573120] Epoch[99] Batch [5]#011Speed: 62.99 samples/sec#011loss=2.553138\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:16 INFO 139797266573120] Epoch[99] Batch[10] avg_epoch_loss=2.548497\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:16 INFO 139797266573120] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=2.54292902946\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:16 INFO 139797266573120] Epoch[99] Batch [10]#011Speed: 62.83 samples/sec#011loss=2.542929\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:16 INFO 139797266573120] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11894.755840301514, \"sum\": 11894.755840301514, \"min\": 11894.755840301514}}, \"EndTime\": 1580271436.497073, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271424.601916}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:16 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.2516688108 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:16 INFO 139797266573120] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:16 INFO 139797266573120] #quality_metric: host=algo-1, epoch=99, train loss <loss>=2.54849741676\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:16 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:18 INFO 139797266573120] Epoch[100] Batch[0] avg_epoch_loss=2.571661\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=2.57166099548\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:23 INFO 139797266573120] Epoch[100] Batch[5] avg_epoch_loss=2.524929\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=2.52492928505\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:23 INFO 139797266573120] Epoch[100] Batch [5]#011Speed: 62.99 samples/sec#011loss=2.524929\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:27 INFO 139797266573120] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10871.402978897095, \"sum\": 10871.402978897095, \"min\": 10871.402978897095}}, \"EndTime\": 1580271447.368837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271436.497137}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:27 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.305815825 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:27 INFO 139797266573120] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=100, train loss <loss>=2.50914051533\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:27 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:29 INFO 139797266573120] Epoch[101] Batch[0] avg_epoch_loss=2.582018\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=2.58201766014\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:34 INFO 139797266573120] Epoch[101] Batch[5] avg_epoch_loss=2.541543\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=2.541543444\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:34 INFO 139797266573120] Epoch[101] Batch [5]#011Speed: 63.01 samples/sec#011loss=2.541543\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:39 INFO 139797266573120] Epoch[101] Batch[10] avg_epoch_loss=2.600413\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=2.67105660439\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:39 INFO 139797266573120] Epoch[101] Batch [10]#011Speed: 62.57 samples/sec#011loss=2.671057\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:39 INFO 139797266573120] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11881.733894348145, \"sum\": 11881.733894348145, \"min\": 11881.733894348145}}, \"EndTime\": 1580271459.250956, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271447.368903}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:39 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.7895258088 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:39 INFO 139797266573120] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=101, train loss <loss>=2.60041306236\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:39 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:40 INFO 139797266573120] Epoch[102] Batch[0] avg_epoch_loss=2.541736\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:40 INFO 139797266573120] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=2.54173636436\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:46 INFO 139797266573120] Epoch[102] Batch[5] avg_epoch_loss=2.581898\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=2.58189789454\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:46 INFO 139797266573120] Epoch[102] Batch [5]#011Speed: 62.65 samples/sec#011loss=2.581898\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:51 INFO 139797266573120] Epoch[102] Batch[10] avg_epoch_loss=2.577116\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=2.57137832642\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:51 INFO 139797266573120] Epoch[102] Batch [10]#011Speed: 62.22 samples/sec#011loss=2.571378\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:51 INFO 139797266573120] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11943.227052688599, \"sum\": 11943.227052688599, \"min\": 11943.227052688599}}, \"EndTime\": 1580271471.194768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271459.251023}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:51 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.3399578787 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:51 INFO 139797266573120] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=102, train loss <loss>=2.57711627267\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:51 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:52 INFO 139797266573120] Epoch[103] Batch[0] avg_epoch_loss=2.589667\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:52 INFO 139797266573120] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=2.58966684341\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:57 INFO 139797266573120] Epoch[103] Batch[5] avg_epoch_loss=2.549225\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:57 INFO 139797266573120] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=2.54922477404\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:17:57 INFO 139797266573120] Epoch[103] Batch [5]#011Speed: 62.66 samples/sec#011loss=2.549225\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:02 INFO 139797266573120] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10898.327112197876, \"sum\": 10898.327112197876, \"min\": 10898.327112197876}}, \"EndTime\": 1580271482.093467, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271471.194837}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:02 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.0444775488 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:02 INFO 139797266573120] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=103, train loss <loss>=2.59234249592\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:02 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:03 INFO 139797266573120] Epoch[104] Batch[0] avg_epoch_loss=2.543427\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:03 INFO 139797266573120] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=2.54342699051\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:18:09 INFO 139797266573120] Epoch[104] Batch[5] avg_epoch_loss=2.534099\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=2.53409874439\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:09 INFO 139797266573120] Epoch[104] Batch [5]#011Speed: 61.55 samples/sec#011loss=2.534099\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:13 INFO 139797266573120] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11092.368841171265, \"sum\": 11092.368841171265, \"min\": 11092.368841171265}}, \"EndTime\": 1580271493.186277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271482.093541}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:13 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.8854627291 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:13 INFO 139797266573120] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:13 INFO 139797266573120] #quality_metric: host=algo-1, epoch=104, train loss <loss>=2.51687965393\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:13 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:14 INFO 139797266573120] Epoch[105] Batch[0] avg_epoch_loss=2.548260\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=2.54825973511\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:20 INFO 139797266573120] Epoch[105] Batch[5] avg_epoch_loss=2.543067\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=2.54306662083\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:20 INFO 139797266573120] Epoch[105] Batch [5]#011Speed: 62.01 samples/sec#011loss=2.543067\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:25 INFO 139797266573120] Epoch[105] Batch[10] avg_epoch_loss=2.598253\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=2.66447649002\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:25 INFO 139797266573120] Epoch[105] Batch [10]#011Speed: 62.26 samples/sec#011loss=2.664476\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:25 INFO 139797266573120] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12026.047945022583, \"sum\": 12026.047945022583, \"min\": 12026.047945022583}}, \"EndTime\": 1580271505.21271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271493.186345}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:25 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.4624813149 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:25 INFO 139797266573120] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=105, train loss <loss>=2.59825292501\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:25 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:26 INFO 139797266573120] Epoch[106] Batch[0] avg_epoch_loss=2.522472\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:26 INFO 139797266573120] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=2.52247238159\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:32 INFO 139797266573120] Epoch[106] Batch[5] avg_epoch_loss=2.508252\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=2.50825202465\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:32 INFO 139797266573120] Epoch[106] Batch [5]#011Speed: 62.29 samples/sec#011loss=2.508252\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:36 INFO 139797266573120] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10953.379154205322, \"sum\": 10953.379154205322, \"min\": 10953.379154205322}}, \"EndTime\": 1580271516.167223, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271505.212777}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:36 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.0637120789 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:36 INFO 139797266573120] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=106, train loss <loss>=2.48379871845\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:36 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:37 INFO 139797266573120] Epoch[107] Batch[0] avg_epoch_loss=2.642552\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=2.64255189896\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:42 INFO 139797266573120] Epoch[107] Batch[5] avg_epoch_loss=2.542271\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:42 INFO 139797266573120] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=2.54227058093\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:42 INFO 139797266573120] Epoch[107] Batch [5]#011Speed: 62.75 samples/sec#011loss=2.542271\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:48 INFO 139797266573120] Epoch[107] Batch[10] avg_epoch_loss=2.466191\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=2.37489459515\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:48 INFO 139797266573120] Epoch[107] Batch [10]#011Speed: 62.56 samples/sec#011loss=2.374895\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:48 INFO 139797266573120] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11919.692993164062, \"sum\": 11919.692993164062, \"min\": 11919.692993164062}}, \"EndTime\": 1580271528.08736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271516.167298}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:48 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.8667240215 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:48 INFO 139797266573120] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=107, train loss <loss>=2.46619058739\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:48 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:49 INFO 139797266573120] Epoch[108] Batch[0] avg_epoch_loss=2.520878\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=2.52087831497\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:54 INFO 139797266573120] Epoch[108] Batch[5] avg_epoch_loss=2.540781\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:54 INFO 139797266573120] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=2.54078050454\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:54 INFO 139797266573120] Epoch[108] Batch [5]#011Speed: 62.35 samples/sec#011loss=2.540781\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:59 INFO 139797266573120] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10930.09901046753, \"sum\": 10930.09901046753, \"min\": 10930.09901046753}}, \"EndTime\": 1580271539.01789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271528.087427}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:59 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.2785926463 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:59 INFO 139797266573120] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:59 INFO 139797266573120] #quality_metric: host=algo-1, epoch=108, train loss <loss>=2.5439347744\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:18:59 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:00 INFO 139797266573120] Epoch[109] Batch[0] avg_epoch_loss=2.561368\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=2.56136775017\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:05 INFO 139797266573120] Epoch[109] Batch[5] avg_epoch_loss=2.565353\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:05 INFO 139797266573120] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=2.56535283724\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:05 INFO 139797266573120] Epoch[109] Batch [5]#011Speed: 62.42 samples/sec#011loss=2.565353\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:11 INFO 139797266573120] Epoch[109] Batch[10] avg_epoch_loss=2.562934\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=2.56003217697\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:11 INFO 139797266573120] Epoch[109] Batch [10]#011Speed: 62.04 samples/sec#011loss=2.560032\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:11 INFO 139797266573120] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12009.80806350708, \"sum\": 12009.80806350708, \"min\": 12009.80806350708}}, \"EndTime\": 1580271551.028161, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271539.018015}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:11 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.5383441942 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:11 INFO 139797266573120] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=109, train loss <loss>=2.5629343553\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:11 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:12 INFO 139797266573120] Epoch[110] Batch[0] avg_epoch_loss=2.495122\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=2.49512219429\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:17 INFO 139797266573120] Epoch[110] Batch[5] avg_epoch_loss=2.512690\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=2.5126897494\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:17 INFO 139797266573120] Epoch[110] Batch [5]#011Speed: 62.01 samples/sec#011loss=2.512690\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:19:23 INFO 139797266573120] Epoch[110] Batch[10] avg_epoch_loss=2.544039\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=2.58165788651\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:23 INFO 139797266573120] Epoch[110] Batch [10]#011Speed: 62.16 samples/sec#011loss=2.581658\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:23 INFO 139797266573120] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12024.817943572998, \"sum\": 12024.817943572998, \"min\": 12024.817943572998}}, \"EndTime\": 1580271563.053389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271551.028223}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:23 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.0544544534 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:23 INFO 139797266573120] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=110, train loss <loss>=2.54403890263\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:23 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:24 INFO 139797266573120] Epoch[111] Batch[0] avg_epoch_loss=2.531157\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=2.53115701675\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:29 INFO 139797266573120] Epoch[111] Batch[5] avg_epoch_loss=2.523680\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=2.52367973328\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:29 INFO 139797266573120] Epoch[111] Batch [5]#011Speed: 62.55 samples/sec#011loss=2.523680\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:35 INFO 139797266573120] Epoch[111] Batch[10] avg_epoch_loss=2.566443\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:35 INFO 139797266573120] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=2.61775856018\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:35 INFO 139797266573120] Epoch[111] Batch [10]#011Speed: 61.94 samples/sec#011loss=2.617759\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:35 INFO 139797266573120] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12016.191959381104, \"sum\": 12016.191959381104, \"min\": 12016.191959381104}}, \"EndTime\": 1580271575.069924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271563.053454}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:35 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.8408850897 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:35 INFO 139797266573120] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:35 INFO 139797266573120] #quality_metric: host=algo-1, epoch=111, train loss <loss>=2.56644283641\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:35 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:36 INFO 139797266573120] Epoch[112] Batch[0] avg_epoch_loss=2.467457\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=2.46745729446\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:41 INFO 139797266573120] Epoch[112] Batch[5] avg_epoch_loss=2.540219\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=2.54021902879\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:41 INFO 139797266573120] Epoch[112] Batch [5]#011Speed: 62.33 samples/sec#011loss=2.540219\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:47 INFO 139797266573120] Epoch[112] Batch[10] avg_epoch_loss=2.577366\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:47 INFO 139797266573120] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=2.6219414711\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:47 INFO 139797266573120] Epoch[112] Batch [10]#011Speed: 62.18 samples/sec#011loss=2.621941\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:47 INFO 139797266573120] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11991.556882858276, \"sum\": 11991.556882858276, \"min\": 11991.556882858276}}, \"EndTime\": 1580271587.061934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271575.069986}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:47 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.8707965537 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:47 INFO 139797266573120] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:47 INFO 139797266573120] #quality_metric: host=algo-1, epoch=112, train loss <loss>=2.57736559348\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:47 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:48 INFO 139797266573120] Epoch[113] Batch[0] avg_epoch_loss=2.596646\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=2.5966463089\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:53 INFO 139797266573120] Epoch[113] Batch[5] avg_epoch_loss=2.537268\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=2.5372676452\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:53 INFO 139797266573120] Epoch[113] Batch [5]#011Speed: 62.46 samples/sec#011loss=2.537268\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:58 INFO 139797266573120] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10940.150022506714, \"sum\": 10940.150022506714, \"min\": 10940.150022506714}}, \"EndTime\": 1580271598.002416, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271587.061993}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:58 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.0371454135 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:58 INFO 139797266573120] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=113, train loss <loss>=2.51909720898\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:58 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:59 INFO 139797266573120] Epoch[114] Batch[0] avg_epoch_loss=2.537561\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:19:59 INFO 139797266573120] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=2.53756093979\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:04 INFO 139797266573120] Epoch[114] Batch[5] avg_epoch_loss=2.543875\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=2.54387482007\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:04 INFO 139797266573120] Epoch[114] Batch [5]#011Speed: 62.43 samples/sec#011loss=2.543875\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:08 INFO 139797266573120] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10979.378938674927, \"sum\": 10979.378938674927, \"min\": 10979.378938674927}}, \"EndTime\": 1580271608.982309, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271598.002476}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:08 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.9243195835 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:08 INFO 139797266573120] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:08 INFO 139797266573120] #quality_metric: host=algo-1, epoch=114, train loss <loss>=2.5414557457\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:08 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:10 INFO 139797266573120] Epoch[115] Batch[0] avg_epoch_loss=2.561891\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=2.56189060211\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:15 INFO 139797266573120] Epoch[115] Batch[5] avg_epoch_loss=2.457907\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:15 INFO 139797266573120] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=2.45790660381\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:15 INFO 139797266573120] Epoch[115] Batch [5]#011Speed: 61.67 samples/sec#011loss=2.457907\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:20 INFO 139797266573120] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11083.258152008057, \"sum\": 11083.258152008057, \"min\": 11083.258152008057}}, \"EndTime\": 1580271620.066082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271608.98238}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:20 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.7667468597 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:20 INFO 139797266573120] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=115, train loss <loss>=2.46774466038\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:20 INFO 139797266573120] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:20:21 INFO 139797266573120] Epoch[116] Batch[0] avg_epoch_loss=2.449689\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=2.44968914986\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:26 INFO 139797266573120] Epoch[116] Batch[5] avg_epoch_loss=2.514868\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:26 INFO 139797266573120] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=2.51486802101\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:26 INFO 139797266573120] Epoch[116] Batch [5]#011Speed: 61.88 samples/sec#011loss=2.514868\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:31 INFO 139797266573120] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11086.788892745972, \"sum\": 11086.788892745972, \"min\": 11086.788892745972}}, \"EndTime\": 1580271631.153376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271620.066156}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:31 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.094486089 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:31 INFO 139797266573120] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:31 INFO 139797266573120] #quality_metric: host=algo-1, epoch=116, train loss <loss>=2.54260537624\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:31 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:32 INFO 139797266573120] Epoch[117] Batch[0] avg_epoch_loss=2.515742\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=2.51574230194\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:37 INFO 139797266573120] Epoch[117] Batch[5] avg_epoch_loss=2.536243\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=2.53624308109\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:37 INFO 139797266573120] Epoch[117] Batch [5]#011Speed: 62.43 samples/sec#011loss=2.536243\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:43 INFO 139797266573120] Epoch[117] Batch[10] avg_epoch_loss=2.524745\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=2.51094799042\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:43 INFO 139797266573120] Epoch[117] Batch [10]#011Speed: 62.04 samples/sec#011loss=2.510948\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:43 INFO 139797266573120] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11985.018968582153, \"sum\": 11985.018968582153, \"min\": 11985.018968582153}}, \"EndTime\": 1580271643.138851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271631.153441}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:43 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.65113631 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:43 INFO 139797266573120] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=117, train loss <loss>=2.5247453126\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:43 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:44 INFO 139797266573120] Epoch[118] Batch[0] avg_epoch_loss=2.522013\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=2.52201318741\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:49 INFO 139797266573120] Epoch[118] Batch[5] avg_epoch_loss=2.560226\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=2.56022616227\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:49 INFO 139797266573120] Epoch[118] Batch [5]#011Speed: 62.53 samples/sec#011loss=2.560226\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:55 INFO 139797266573120] Epoch[118] Batch[10] avg_epoch_loss=2.549962\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=2.53764576912\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:55 INFO 139797266573120] Epoch[118] Batch [10]#011Speed: 62.15 samples/sec#011loss=2.537646\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:55 INFO 139797266573120] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11949.653148651123, \"sum\": 11949.653148651123, \"min\": 11949.653148651123}}, \"EndTime\": 1580271655.088966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271643.138915}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:55 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.7249917884 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:55 INFO 139797266573120] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=118, train loss <loss>=2.5499623472\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:55 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:56 INFO 139797266573120] Epoch[119] Batch[0] avg_epoch_loss=2.426165\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:20:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=2.42616486549\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:01 INFO 139797266573120] Epoch[119] Batch[5] avg_epoch_loss=2.496170\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:01 INFO 139797266573120] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=2.49617024263\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:01 INFO 139797266573120] Epoch[119] Batch [5]#011Speed: 62.16 samples/sec#011loss=2.496170\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:06 INFO 139797266573120] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10981.931924819946, \"sum\": 10981.931924819946, \"min\": 10981.931924819946}}, \"EndTime\": 1580271666.071303, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271655.089028}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:06 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.9111593321 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:06 INFO 139797266573120] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=119, train loss <loss>=2.51918904781\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:06 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:07 INFO 139797266573120] Epoch[120] Batch[0] avg_epoch_loss=2.512284\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=2.51228427887\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:12 INFO 139797266573120] Epoch[120] Batch[5] avg_epoch_loss=2.554396\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=2.55439555645\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:12 INFO 139797266573120] Epoch[120] Batch [5]#011Speed: 61.54 samples/sec#011loss=2.554396\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:17 INFO 139797266573120] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11046.159029006958, \"sum\": 11046.159029006958, \"min\": 11046.159029006958}}, \"EndTime\": 1580271677.118098, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271666.071363}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:17 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.8518109141 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:17 INFO 139797266573120] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=120, train loss <loss>=2.54496905804\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:17 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:18 INFO 139797266573120] Epoch[121] Batch[0] avg_epoch_loss=2.603956\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=2.60395598412\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:24 INFO 139797266573120] Epoch[121] Batch[5] avg_epoch_loss=2.543705\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=2.54370510578\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:24 INFO 139797266573120] Epoch[121] Batch [5]#011Speed: 61.61 samples/sec#011loss=2.543705\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] Epoch[121] Batch[10] avg_epoch_loss=2.438597\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=2.31246767044\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] Epoch[121] Batch [10]#011Speed: 61.20 samples/sec#011loss=2.312468\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12127.050876617432, \"sum\": 12127.050876617432, \"min\": 12127.050876617432}}, \"EndTime\": 1580271689.245722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271677.118163}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.1040209244 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=121, train loss <loss>=2.43859718063\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:29 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/state_adf548d0-ea3e-450f-ac7e-03b80a46999d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 211.90404891967773, \"sum\": 211.90404891967773, \"min\": 211.90404891967773}}, \"EndTime\": 1580271689.458148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271689.245779}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:31 INFO 139797266573120] Epoch[122] Batch[0] avg_epoch_loss=2.467630\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:31 INFO 139797266573120] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=2.46762990952\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:21:36 INFO 139797266573120] Epoch[122] Batch[5] avg_epoch_loss=2.507869\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=2.50786884626\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:36 INFO 139797266573120] Epoch[122] Batch [5]#011Speed: 62.35 samples/sec#011loss=2.507869\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:41 INFO 139797266573120] Epoch[122] Batch[10] avg_epoch_loss=2.467563\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=2.41919641495\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:41 INFO 139797266573120] Epoch[122] Batch [10]#011Speed: 62.16 samples/sec#011loss=2.419196\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:41 INFO 139797266573120] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12009.521961212158, \"sum\": 12009.521961212158, \"min\": 12009.521961212158}}, \"EndTime\": 1580271701.467798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271689.458215}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:41 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.2065629344 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:41 INFO 139797266573120] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:41 INFO 139797266573120] #quality_metric: host=algo-1, epoch=122, train loss <loss>=2.46756319566\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:41 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:43 INFO 139797266573120] Epoch[123] Batch[0] avg_epoch_loss=2.571373\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=2.571372509\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:48 INFO 139797266573120] Epoch[123] Batch[5] avg_epoch_loss=2.512294\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=2.51229425271\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:48 INFO 139797266573120] Epoch[123] Batch [5]#011Speed: 62.46 samples/sec#011loss=2.512294\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:53 INFO 139797266573120] Epoch[123] Batch[10] avg_epoch_loss=2.519580\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=2.52832231522\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:53 INFO 139797266573120] Epoch[123] Batch [10]#011Speed: 62.55 samples/sec#011loss=2.528322\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:53 INFO 139797266573120] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11959.221839904785, \"sum\": 11959.221839904785, \"min\": 11959.221839904785}}, \"EndTime\": 1580271713.427508, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271701.467856}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:53 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.1069212029 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:53 INFO 139797266573120] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=123, train loss <loss>=2.51957973567\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:53 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:55 INFO 139797266573120] Epoch[124] Batch[0] avg_epoch_loss=2.441691\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:21:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=2.4416911602\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:00 INFO 139797266573120] Epoch[124] Batch[5] avg_epoch_loss=2.513098\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=2.5130978028\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:00 INFO 139797266573120] Epoch[124] Batch [5]#011Speed: 62.48 samples/sec#011loss=2.513098\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:04 INFO 139797266573120] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10920.891046524048, \"sum\": 10920.891046524048, \"min\": 10920.891046524048}}, \"EndTime\": 1580271724.348839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271713.427564}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:04 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.9473296201 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:04 INFO 139797266573120] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=124, train loss <loss>=2.51430642605\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:04 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:06 INFO 139797266573120] Epoch[125] Batch[0] avg_epoch_loss=2.445193\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=2.44519329071\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:11 INFO 139797266573120] Epoch[125] Batch[5] avg_epoch_loss=2.506631\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=2.50663050016\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:11 INFO 139797266573120] Epoch[125] Batch [5]#011Speed: 62.05 samples/sec#011loss=2.506631\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:15 INFO 139797266573120] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10978.888034820557, \"sum\": 10978.888034820557, \"min\": 10978.888034820557}}, \"EndTime\": 1580271735.328122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271724.348904}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:15 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.7407543558 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:15 INFO 139797266573120] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:15 INFO 139797266573120] #quality_metric: host=algo-1, epoch=125, train loss <loss>=2.56276028156\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:15 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:17 INFO 139797266573120] Epoch[126] Batch[0] avg_epoch_loss=2.503386\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=2.50338554382\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:22 INFO 139797266573120] Epoch[126] Batch[5] avg_epoch_loss=2.499801\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=2.49980080128\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:22 INFO 139797266573120] Epoch[126] Batch [5]#011Speed: 62.71 samples/sec#011loss=2.499801\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:26 INFO 139797266573120] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10906.893968582153, \"sum\": 10906.893968582153, \"min\": 10906.893968582153}}, \"EndTime\": 1580271746.235587, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271735.328181}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:26 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.1271518471 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:26 INFO 139797266573120] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:26 INFO 139797266573120] #quality_metric: host=algo-1, epoch=126, train loss <loss>=2.49690856934\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:26 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:27 INFO 139797266573120] Epoch[127] Batch[0] avg_epoch_loss=2.570956\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=2.57095623016\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:33 INFO 139797266573120] Epoch[127] Batch[5] avg_epoch_loss=2.518262\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:33 INFO 139797266573120] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=2.51826155186\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:33 INFO 139797266573120] Epoch[127] Batch [5]#011Speed: 62.77 samples/sec#011loss=2.518262\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:37 INFO 139797266573120] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10907.529830932617, \"sum\": 10907.529830932617, \"min\": 10907.529830932617}}, \"EndTime\": 1580271757.143757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271746.235784}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:37 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.1244405127 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:37 INFO 139797266573120] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:37 INFO 139797266573120] #quality_metric: host=algo-1, epoch=127, train loss <loss>=2.55027880669\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:37 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:38 INFO 139797266573120] Epoch[128] Batch[0] avg_epoch_loss=2.541612\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=2.5416123867\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:22:43 INFO 139797266573120] Epoch[128] Batch[5] avg_epoch_loss=2.477312\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=2.47731248538\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:43 INFO 139797266573120] Epoch[128] Batch [5]#011Speed: 62.79 samples/sec#011loss=2.477312\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:48 INFO 139797266573120] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10892.277002334595, \"sum\": 10892.277002334595, \"min\": 10892.277002334595}}, \"EndTime\": 1580271768.036551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271757.143823}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:48 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.0220854268 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:48 INFO 139797266573120] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=128, train loss <loss>=2.49412462711\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:48 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:49 INFO 139797266573120] Epoch[129] Batch[0] avg_epoch_loss=2.421614\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=2.42161369324\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:54 INFO 139797266573120] Epoch[129] Batch[5] avg_epoch_loss=2.470083\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:54 INFO 139797266573120] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=2.47008256118\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:22:54 INFO 139797266573120] Epoch[129] Batch [5]#011Speed: 62.37 samples/sec#011loss=2.470083\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:00 INFO 139797266573120] Epoch[129] Batch[10] avg_epoch_loss=2.503494\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=2.54358744621\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:00 INFO 139797266573120] Epoch[129] Batch [10]#011Speed: 62.24 samples/sec#011loss=2.543587\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:00 INFO 139797266573120] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11988.019943237305, \"sum\": 11988.019943237305, \"min\": 11988.019943237305}}, \"EndTime\": 1580271780.02496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271768.036627}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:00 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.9721120509 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:00 INFO 139797266573120] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=129, train loss <loss>=2.50349387256\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:00 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:01 INFO 139797266573120] Epoch[130] Batch[0] avg_epoch_loss=2.523291\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:01 INFO 139797266573120] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=2.52329134941\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:06 INFO 139797266573120] Epoch[130] Batch[5] avg_epoch_loss=2.567559\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=2.56755880515\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:06 INFO 139797266573120] Epoch[130] Batch [5]#011Speed: 62.03 samples/sec#011loss=2.567559\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:12 INFO 139797266573120] Epoch[130] Batch[10] avg_epoch_loss=2.483529\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=2.38269407749\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:12 INFO 139797266573120] Epoch[130] Batch [10]#011Speed: 61.67 samples/sec#011loss=2.382694\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:12 INFO 139797266573120] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12105.458974838257, \"sum\": 12105.458974838257, \"min\": 12105.458974838257}}, \"EndTime\": 1580271792.130897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271780.025024}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:12 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.8508739559 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:12 INFO 139797266573120] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=130, train loss <loss>=2.48352938349\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:12 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:13 INFO 139797266573120] Epoch[131] Batch[0] avg_epoch_loss=2.582023\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:13 INFO 139797266573120] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=2.58202266693\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:18 INFO 139797266573120] Epoch[131] Batch[5] avg_epoch_loss=2.506089\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=2.50608869394\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:18 INFO 139797266573120] Epoch[131] Batch [5]#011Speed: 62.30 samples/sec#011loss=2.506089\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:23 INFO 139797266573120] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10970.024108886719, \"sum\": 10970.024108886719, \"min\": 10970.024108886719}}, \"EndTime\": 1580271803.101353, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271792.130959}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:23 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.2435641793 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:23 INFO 139797266573120] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=131, train loss <loss>=2.52976565361\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:23 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:24 INFO 139797266573120] Epoch[132] Batch[0] avg_epoch_loss=2.522643\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:24 INFO 139797266573120] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=2.52264285088\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:29 INFO 139797266573120] Epoch[132] Batch[5] avg_epoch_loss=2.515412\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=2.5154115359\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:29 INFO 139797266573120] Epoch[132] Batch [5]#011Speed: 62.15 samples/sec#011loss=2.515412\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:34 INFO 139797266573120] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10964.71381187439, \"sum\": 10964.71381187439, \"min\": 10964.71381187439}}, \"EndTime\": 1580271814.066652, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271803.101438}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:34 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.2709205421 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:34 INFO 139797266573120] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=132, train loss <loss>=2.57527544498\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:34 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:35 INFO 139797266573120] Epoch[133] Batch[0] avg_epoch_loss=2.530271\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:35 INFO 139797266573120] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=2.5302708149\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:40 INFO 139797266573120] Epoch[133] Batch[5] avg_epoch_loss=2.472673\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:40 INFO 139797266573120] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=2.47267274062\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:40 INFO 139797266573120] Epoch[133] Batch [5]#011Speed: 62.61 samples/sec#011loss=2.472673\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:46 INFO 139797266573120] Epoch[133] Batch[10] avg_epoch_loss=2.486685\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=2.50349917412\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:46 INFO 139797266573120] Epoch[133] Batch [10]#011Speed: 62.17 samples/sec#011loss=2.503499\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:46 INFO 139797266573120] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11955.295085906982, \"sum\": 11955.295085906982, \"min\": 11955.295085906982}}, \"EndTime\": 1580271826.022323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271814.066717}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:46 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.7071145502 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:46 INFO 139797266573120] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:46 INFO 139797266573120] #quality_metric: host=algo-1, epoch=133, train loss <loss>=2.48668475585\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:46 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:47 INFO 139797266573120] Epoch[134] Batch[0] avg_epoch_loss=2.427634\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:47 INFO 139797266573120] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=2.42763352394\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:23:52 INFO 139797266573120] Epoch[134] Batch[5] avg_epoch_loss=2.525973\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:52 INFO 139797266573120] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=2.52597339948\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:52 INFO 139797266573120] Epoch[134] Batch [5]#011Speed: 62.67 samples/sec#011loss=2.525973\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:57 INFO 139797266573120] Epoch[134] Batch[10] avg_epoch_loss=2.480496\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:57 INFO 139797266573120] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=2.42592298985\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:57 INFO 139797266573120] Epoch[134] Batch [10]#011Speed: 62.33 samples/sec#011loss=2.425923\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:57 INFO 139797266573120] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11967.622995376587, \"sum\": 11967.622995376587, \"min\": 11967.622995376587}}, \"EndTime\": 1580271837.990278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271826.022386}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:57 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.3128046569 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:57 INFO 139797266573120] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:57 INFO 139797266573120] #quality_metric: host=algo-1, epoch=134, train loss <loss>=2.48049594056\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:57 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:59 INFO 139797266573120] Epoch[135] Batch[0] avg_epoch_loss=2.460662\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:23:59 INFO 139797266573120] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=2.46066236496\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:04 INFO 139797266573120] Epoch[135] Batch[5] avg_epoch_loss=2.552716\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=2.55271601677\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:04 INFO 139797266573120] Epoch[135] Batch [5]#011Speed: 62.34 samples/sec#011loss=2.552716\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:10 INFO 139797266573120] Epoch[135] Batch[10] avg_epoch_loss=2.559455\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=2.56754207611\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:10 INFO 139797266573120] Epoch[135] Batch [10]#011Speed: 61.17 samples/sec#011loss=2.567542\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:10 INFO 139797266573120] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12071.367025375366, \"sum\": 12071.367025375366, \"min\": 12071.367025375366}}, \"EndTime\": 1580271850.062167, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271837.990337}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:10 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.1714594004 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:10 INFO 139797266573120] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=135, train loss <loss>=2.55945513465\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:10 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:11 INFO 139797266573120] Epoch[136] Batch[0] avg_epoch_loss=2.484060\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:11 INFO 139797266573120] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=2.48406028748\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:16 INFO 139797266573120] Epoch[136] Batch[5] avg_epoch_loss=2.564418\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:16 INFO 139797266573120] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=2.56441764037\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:16 INFO 139797266573120] Epoch[136] Batch [5]#011Speed: 62.26 samples/sec#011loss=2.564418\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:21 INFO 139797266573120] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10982.847929000854, \"sum\": 10982.847929000854, \"min\": 10982.847929000854}}, \"EndTime\": 1580271861.045572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271850.062224}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:21 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.088523239 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:21 INFO 139797266573120] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=136, train loss <loss>=2.54023416042\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:21 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:22 INFO 139797266573120] Epoch[137] Batch[0] avg_epoch_loss=2.569996\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=2.56999564171\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:27 INFO 139797266573120] Epoch[137] Batch[5] avg_epoch_loss=2.509580\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=2.50958013535\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:27 INFO 139797266573120] Epoch[137] Batch [5]#011Speed: 62.63 samples/sec#011loss=2.509580\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:32 INFO 139797266573120] Epoch[137] Batch[10] avg_epoch_loss=2.439170\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=2.35467829704\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:32 INFO 139797266573120] Epoch[137] Batch [10]#011Speed: 62.55 samples/sec#011loss=2.354678\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:32 INFO 139797266573120] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11943.38083267212, \"sum\": 11943.38083267212, \"min\": 11943.38083267212}}, \"EndTime\": 1580271872.989373, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271861.045636}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:32 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.0044014371 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:32 INFO 139797266573120] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=137, train loss <loss>=2.43917020884\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:32 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:34 INFO 139797266573120] Epoch[138] Batch[0] avg_epoch_loss=2.571872\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:34 INFO 139797266573120] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=2.57187247276\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:39 INFO 139797266573120] Epoch[138] Batch[5] avg_epoch_loss=2.508586\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:39 INFO 139797266573120] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=2.50858608882\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:39 INFO 139797266573120] Epoch[138] Batch [5]#011Speed: 62.58 samples/sec#011loss=2.508586\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:43 INFO 139797266573120] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10939.77403640747, \"sum\": 10939.77403640747, \"min\": 10939.77403640747}}, \"EndTime\": 1580271883.929524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271872.989435}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:43 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.587527255 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:43 INFO 139797266573120] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=138, train loss <loss>=2.51689212322\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:43 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:45 INFO 139797266573120] Epoch[139] Batch[0] avg_epoch_loss=2.622066\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:45 INFO 139797266573120] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=2.62206554413\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:50 INFO 139797266573120] Epoch[139] Batch[5] avg_epoch_loss=2.547439\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:50 INFO 139797266573120] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=2.54743850231\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:50 INFO 139797266573120] Epoch[139] Batch [5]#011Speed: 62.43 samples/sec#011loss=2.547439\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:24:55 INFO 139797266573120] Epoch[139] Batch[10] avg_epoch_loss=2.455846\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=2.34593527317\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:55 INFO 139797266573120] Epoch[139] Batch [10]#011Speed: 62.32 samples/sec#011loss=2.345935\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:55 INFO 139797266573120] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11943.78399848938, \"sum\": 11943.78399848938, \"min\": 11943.78399848938}}, \"EndTime\": 1580271895.87371, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271883.929589}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:55 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.4212227106 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:55 INFO 139797266573120] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=139, train loss <loss>=2.45584612543\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:55 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:57 INFO 139797266573120] Epoch[140] Batch[0] avg_epoch_loss=2.557323\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:24:57 INFO 139797266573120] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=2.55732274055\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:02 INFO 139797266573120] Epoch[140] Batch[5] avg_epoch_loss=2.533341\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=2.53334097068\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:02 INFO 139797266573120] Epoch[140] Batch [5]#011Speed: 61.96 samples/sec#011loss=2.533341\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:07 INFO 139797266573120] Epoch[140] Batch[10] avg_epoch_loss=2.540197\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=2.54842386246\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:07 INFO 139797266573120] Epoch[140] Batch [10]#011Speed: 61.84 samples/sec#011loss=2.548424\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:07 INFO 139797266573120] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12054.749011993408, \"sum\": 12054.749011993408, \"min\": 12054.749011993408}}, \"EndTime\": 1580271907.929093, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271895.873763}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:07 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=54.9986557404 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:07 INFO 139797266573120] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=140, train loss <loss>=2.54019683058\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:07 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:09 INFO 139797266573120] Epoch[141] Batch[0] avg_epoch_loss=2.597694\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=2.59769439697\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:14 INFO 139797266573120] Epoch[141] Batch[5] avg_epoch_loss=2.496151\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=2.49615148703\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:14 INFO 139797266573120] Epoch[141] Batch [5]#011Speed: 61.97 samples/sec#011loss=2.496151\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:19 INFO 139797266573120] Epoch[141] Batch[10] avg_epoch_loss=2.500883\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:19 INFO 139797266573120] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=2.50656061172\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:19 INFO 139797266573120] Epoch[141] Batch [10]#011Speed: 62.40 samples/sec#011loss=2.506561\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:21 INFO 139797266573120] processed a total of 707 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 13104.862928390503, \"sum\": 13104.862928390503, \"min\": 13104.862928390503}}, \"EndTime\": 1580271921.034342, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271907.929154}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:21 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.9489892313 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:21 INFO 139797266573120] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=141, train loss <loss>=2.45112494628\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:21 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:22 INFO 139797266573120] Epoch[142] Batch[0] avg_epoch_loss=2.468536\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:22 INFO 139797266573120] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=2.46853590012\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:27 INFO 139797266573120] Epoch[142] Batch[5] avg_epoch_loss=2.506813\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=2.50681312879\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:27 INFO 139797266573120] Epoch[142] Batch [5]#011Speed: 61.99 samples/sec#011loss=2.506813\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:32 INFO 139797266573120] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11010.50615310669, \"sum\": 11010.50615310669, \"min\": 11010.50615310669}}, \"EndTime\": 1580271932.04529, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271921.034418}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:32 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.6713880102 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:32 INFO 139797266573120] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=142, train loss <loss>=2.51993994713\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:32 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:33 INFO 139797266573120] Epoch[143] Batch[0] avg_epoch_loss=2.638381\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:33 INFO 139797266573120] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=2.6383805275\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:38 INFO 139797266573120] Epoch[143] Batch[5] avg_epoch_loss=2.498964\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=2.49896423022\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:38 INFO 139797266573120] Epoch[143] Batch [5]#011Speed: 62.35 samples/sec#011loss=2.498964\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:44 INFO 139797266573120] Epoch[143] Batch[10] avg_epoch_loss=2.471908\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=2.43943963051\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:44 INFO 139797266573120] Epoch[143] Batch [10]#011Speed: 61.65 samples/sec#011loss=2.439440\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:44 INFO 139797266573120] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12038.58995437622, \"sum\": 12038.58995437622, \"min\": 12038.58995437622}}, \"EndTime\": 1580271944.084429, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271932.045374}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:44 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.6603592371 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:44 INFO 139797266573120] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:44 INFO 139797266573120] #quality_metric: host=algo-1, epoch=143, train loss <loss>=2.47190759399\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:44 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:45 INFO 139797266573120] Epoch[144] Batch[0] avg_epoch_loss=2.675503\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:45 INFO 139797266573120] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=2.67550349236\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:50 INFO 139797266573120] Epoch[144] Batch[5] avg_epoch_loss=2.550132\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:50 INFO 139797266573120] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=2.55013211568\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:50 INFO 139797266573120] Epoch[144] Batch [5]#011Speed: 62.13 samples/sec#011loss=2.550132\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:55 INFO 139797266573120] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10979.709148406982, \"sum\": 10979.709148406982, \"min\": 10979.709148406982}}, \"EndTime\": 1580271955.064575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271944.084486}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:55 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.0155758643 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:55 INFO 139797266573120] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=144, train loss <loss>=2.54209589958\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:55 INFO 139797266573120] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:25:56 INFO 139797266573120] Epoch[145] Batch[0] avg_epoch_loss=2.531512\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:25:56 INFO 139797266573120] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=2.5315117836\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:01 INFO 139797266573120] Epoch[145] Batch[5] avg_epoch_loss=2.481858\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:01 INFO 139797266573120] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=2.48185801506\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:01 INFO 139797266573120] Epoch[145] Batch [5]#011Speed: 62.03 samples/sec#011loss=2.481858\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:07 INFO 139797266573120] Epoch[145] Batch[10] avg_epoch_loss=2.460518\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=2.43490986824\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:07 INFO 139797266573120] Epoch[145] Batch [10]#011Speed: 61.54 samples/sec#011loss=2.434910\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:07 INFO 139797266573120] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12056.360006332397, \"sum\": 12056.360006332397, \"min\": 12056.360006332397}}, \"EndTime\": 1580271967.1215, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271955.064642}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:07 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.6639585337 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:07 INFO 139797266573120] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=145, train loss <loss>=2.46051794832\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:07 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:08 INFO 139797266573120] Epoch[146] Batch[0] avg_epoch_loss=2.502516\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:08 INFO 139797266573120] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=2.50251626968\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:14 INFO 139797266573120] Epoch[146] Batch[5] avg_epoch_loss=2.502382\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=2.50238231818\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:14 INFO 139797266573120] Epoch[146] Batch [5]#011Speed: 62.18 samples/sec#011loss=2.502382\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:19 INFO 139797266573120] Epoch[146] Batch[10] avg_epoch_loss=2.532887\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:19 INFO 139797266573120] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=2.56949186325\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:19 INFO 139797266573120] Epoch[146] Batch [10]#011Speed: 62.37 samples/sec#011loss=2.569492\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:19 INFO 139797266573120] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12020.434141159058, \"sum\": 12020.434141159058, \"min\": 12020.434141159058}}, \"EndTime\": 1580271979.142486, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271967.121559}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:19 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.8211748803 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:19 INFO 139797266573120] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:19 INFO 139797266573120] #quality_metric: host=algo-1, epoch=146, train loss <loss>=2.53288665685\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:19 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:20 INFO 139797266573120] Epoch[147] Batch[0] avg_epoch_loss=2.547143\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:20 INFO 139797266573120] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=2.54714298248\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:25 INFO 139797266573120] Epoch[147] Batch[5] avg_epoch_loss=2.501736\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=2.50173604488\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:25 INFO 139797266573120] Epoch[147] Batch [5]#011Speed: 62.54 samples/sec#011loss=2.501736\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:30 INFO 139797266573120] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10958.127975463867, \"sum\": 10958.127975463867, \"min\": 10958.127975463867}}, \"EndTime\": 1580271990.101008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271979.142549}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:30 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.3997182641 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:30 INFO 139797266573120] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:30 INFO 139797266573120] #quality_metric: host=algo-1, epoch=147, train loss <loss>=2.52534253597\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:30 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:31 INFO 139797266573120] Epoch[148] Batch[0] avg_epoch_loss=2.472339\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:31 INFO 139797266573120] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=2.47233867645\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:36 INFO 139797266573120] Epoch[148] Batch[5] avg_epoch_loss=2.523159\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=2.52315870921\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:36 INFO 139797266573120] Epoch[148] Batch [5]#011Speed: 62.44 samples/sec#011loss=2.523159\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:42 INFO 139797266573120] Epoch[148] Batch[10] avg_epoch_loss=2.513222\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:42 INFO 139797266573120] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=2.50129771233\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:42 INFO 139797266573120] Epoch[148] Batch [10]#011Speed: 62.17 samples/sec#011loss=2.501298\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:42 INFO 139797266573120] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12001.930952072144, \"sum\": 12001.930952072144, \"min\": 12001.930952072144}}, \"EndTime\": 1580272002.103501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580271990.101085}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:42 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.3239517624 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:42 INFO 139797266573120] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:42 INFO 139797266573120] #quality_metric: host=algo-1, epoch=148, train loss <loss>=2.51322189244\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:42 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:43 INFO 139797266573120] Epoch[149] Batch[0] avg_epoch_loss=2.455538\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=2.45553827286\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:48 INFO 139797266573120] Epoch[149] Batch[5] avg_epoch_loss=2.442232\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:48 INFO 139797266573120] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=2.44223201275\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:48 INFO 139797266573120] Epoch[149] Batch [5]#011Speed: 62.46 samples/sec#011loss=2.442232\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:54 INFO 139797266573120] Epoch[149] Batch[10] avg_epoch_loss=2.479245\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:54 INFO 139797266573120] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=2.52365970612\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:54 INFO 139797266573120] Epoch[149] Batch [10]#011Speed: 62.04 samples/sec#011loss=2.523660\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:54 INFO 139797266573120] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12004.479169845581, \"sum\": 12004.479169845581, \"min\": 12004.479169845581}}, \"EndTime\": 1580272014.108419, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272002.10357}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:54 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.3122366864 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:54 INFO 139797266573120] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:54 INFO 139797266573120] #quality_metric: host=algo-1, epoch=149, train loss <loss>=2.47924460064\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:54 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:55 INFO 139797266573120] Epoch[150] Batch[0] avg_epoch_loss=2.455988\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:26:55 INFO 139797266573120] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=2.45598769188\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:00 INFO 139797266573120] Epoch[150] Batch[5] avg_epoch_loss=2.509099\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=2.50909920533\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:00 INFO 139797266573120] Epoch[150] Batch [5]#011Speed: 62.38 samples/sec#011loss=2.509099\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:27:06 INFO 139797266573120] Epoch[150] Batch[10] avg_epoch_loss=2.469075\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=2.42104635239\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:06 INFO 139797266573120] Epoch[150] Batch [10]#011Speed: 62.06 samples/sec#011loss=2.421046\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:06 INFO 139797266573120] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12012.593984603882, \"sum\": 12012.593984603882, \"min\": 12012.593984603882}}, \"EndTime\": 1580272026.121459, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272014.108482}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:06 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.1084078298 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:06 INFO 139797266573120] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:06 INFO 139797266573120] #quality_metric: host=algo-1, epoch=150, train loss <loss>=2.46907518127\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:06 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:07 INFO 139797266573120] Epoch[151] Batch[0] avg_epoch_loss=2.374687\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:07 INFO 139797266573120] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=2.37468719482\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:12 INFO 139797266573120] Epoch[151] Batch[5] avg_epoch_loss=2.482321\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:12 INFO 139797266573120] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=2.48232134183\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:12 INFO 139797266573120] Epoch[151] Batch [5]#011Speed: 62.50 samples/sec#011loss=2.482321\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:17 INFO 139797266573120] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10904.669046401978, \"sum\": 10904.669046401978, \"min\": 10904.669046401978}}, \"EndTime\": 1580272037.026466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272026.121521}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:17 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=58.6899607925 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:17 INFO 139797266573120] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:17 INFO 139797266573120] #quality_metric: host=algo-1, epoch=151, train loss <loss>=2.47951769829\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:17 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:18 INFO 139797266573120] Epoch[152] Batch[0] avg_epoch_loss=2.469511\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:18 INFO 139797266573120] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=2.46951079369\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:23 INFO 139797266573120] Epoch[152] Batch[5] avg_epoch_loss=2.483133\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:23 INFO 139797266573120] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=2.48313347499\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:23 INFO 139797266573120] Epoch[152] Batch [5]#011Speed: 62.47 samples/sec#011loss=2.483133\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:29 INFO 139797266573120] Epoch[152] Batch[10] avg_epoch_loss=2.496482\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=2.51250033379\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:29 INFO 139797266573120] Epoch[152] Batch [10]#011Speed: 62.34 samples/sec#011loss=2.512500\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:29 INFO 139797266573120] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11982.254981994629, \"sum\": 11982.254981994629, \"min\": 11982.254981994629}}, \"EndTime\": 1580272049.009222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272037.02653}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:29 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.5831158858 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:29 INFO 139797266573120] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:29 INFO 139797266573120] #quality_metric: host=algo-1, epoch=152, train loss <loss>=2.49648204717\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:29 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:30 INFO 139797266573120] Epoch[153] Batch[0] avg_epoch_loss=2.393803\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:30 INFO 139797266573120] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=2.39380335808\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:35 INFO 139797266573120] Epoch[153] Batch[5] avg_epoch_loss=2.471022\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:35 INFO 139797266573120] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=2.47102220853\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:35 INFO 139797266573120] Epoch[153] Batch [5]#011Speed: 62.99 samples/sec#011loss=2.471022\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:40 INFO 139797266573120] Epoch[153] Batch[10] avg_epoch_loss=2.455207\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:40 INFO 139797266573120] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=2.43622951508\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:40 INFO 139797266573120] Epoch[153] Batch [10]#011Speed: 62.81 samples/sec#011loss=2.436230\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:40 INFO 139797266573120] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11871.381998062134, \"sum\": 11871.381998062134, \"min\": 11871.381998062134}}, \"EndTime\": 1580272060.881104, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272049.009307}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:40 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.0900561486 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:40 INFO 139797266573120] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:40 INFO 139797266573120] #quality_metric: host=algo-1, epoch=153, train loss <loss>=2.45520734787\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:40 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:42 INFO 139797266573120] Epoch[154] Batch[0] avg_epoch_loss=2.525039\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:42 INFO 139797266573120] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=2.52503943443\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:47 INFO 139797266573120] Epoch[154] Batch[5] avg_epoch_loss=2.508863\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:47 INFO 139797266573120] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=2.50886253516\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:47 INFO 139797266573120] Epoch[154] Batch [5]#011Speed: 62.66 samples/sec#011loss=2.508863\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:51 INFO 139797266573120] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10895.368099212646, \"sum\": 10895.368099212646, \"min\": 10895.368099212646}}, \"EndTime\": 1580272071.776826, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272060.881164}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:51 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.6290708868 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:51 INFO 139797266573120] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:51 INFO 139797266573120] #quality_metric: host=algo-1, epoch=154, train loss <loss>=2.49815762043\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:51 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:53 INFO 139797266573120] Epoch[155] Batch[0] avg_epoch_loss=2.431214\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:53 INFO 139797266573120] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=2.43121361732\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:58 INFO 139797266573120] Epoch[155] Batch[5] avg_epoch_loss=2.495954\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=2.49595367908\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:27:58 INFO 139797266573120] Epoch[155] Batch [5]#011Speed: 62.87 samples/sec#011loss=2.495954\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:02 INFO 139797266573120] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10924.032926559448, \"sum\": 10924.032926559448, \"min\": 10924.032926559448}}, \"EndTime\": 1580272082.70129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272071.776894}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:02 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.7550919025 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:02 INFO 139797266573120] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:02 INFO 139797266573120] #quality_metric: host=algo-1, epoch=155, train loss <loss>=2.50226583481\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:02 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:04 INFO 139797266573120] Epoch[156] Batch[0] avg_epoch_loss=2.708228\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:04 INFO 139797266573120] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=2.70822834969\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01/29/2020 04:28:09 INFO 139797266573120] Epoch[156] Batch[5] avg_epoch_loss=2.516651\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:09 INFO 139797266573120] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=2.51665103436\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:09 INFO 139797266573120] Epoch[156] Batch [5]#011Speed: 62.29 samples/sec#011loss=2.516651\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:14 INFO 139797266573120] Epoch[156] Batch[10] avg_epoch_loss=2.522857\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=2.53030352592\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:14 INFO 139797266573120] Epoch[156] Batch [10]#011Speed: 61.32 samples/sec#011loss=2.530304\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:14 INFO 139797266573120] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12050.902128219604, \"sum\": 12050.902128219604, \"min\": 12050.902128219604}}, \"EndTime\": 1580272094.752579, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272082.701357}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:14 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=53.190641846 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:14 INFO 139797266573120] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:14 INFO 139797266573120] #quality_metric: host=algo-1, epoch=156, train loss <loss>=2.52285671234\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:14 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:16 INFO 139797266573120] Epoch[157] Batch[0] avg_epoch_loss=2.643321\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:16 INFO 139797266573120] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=2.64332103729\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:21 INFO 139797266573120] Epoch[157] Batch[5] avg_epoch_loss=2.520700\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:21 INFO 139797266573120] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=2.52069993814\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:21 INFO 139797266573120] Epoch[157] Batch [5]#011Speed: 61.45 samples/sec#011loss=2.520700\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:25 INFO 139797266573120] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11022.270202636719, \"sum\": 11022.270202636719, \"min\": 11022.270202636719}}, \"EndTime\": 1580272105.775333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272094.752637}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:25 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.3379255296 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:25 INFO 139797266573120] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:25 INFO 139797266573120] #quality_metric: host=algo-1, epoch=157, train loss <loss>=2.51734750271\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:25 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:27 INFO 139797266573120] Epoch[158] Batch[0] avg_epoch_loss=2.412181\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:27 INFO 139797266573120] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=2.41218113899\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:32 INFO 139797266573120] Epoch[158] Batch[5] avg_epoch_loss=2.520778\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:32 INFO 139797266573120] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=2.5207776626\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:32 INFO 139797266573120] Epoch[158] Batch [5]#011Speed: 63.01 samples/sec#011loss=2.520778\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:36 INFO 139797266573120] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10877.85005569458, \"sum\": 10877.85005569458, \"min\": 10877.85005569458}}, \"EndTime\": 1580272116.653624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272105.775402}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:36 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.2606575379 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:36 INFO 139797266573120] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:36 INFO 139797266573120] #quality_metric: host=algo-1, epoch=158, train loss <loss>=2.54272663593\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:36 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:38 INFO 139797266573120] Epoch[159] Batch[0] avg_epoch_loss=2.492107\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:38 INFO 139797266573120] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=2.49210691452\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:43 INFO 139797266573120] Epoch[159] Batch[5] avg_epoch_loss=2.477260\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:43 INFO 139797266573120] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=2.47725975513\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:43 INFO 139797266573120] Epoch[159] Batch [5]#011Speed: 62.49 samples/sec#011loss=2.477260\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:47 INFO 139797266573120] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11007.742166519165, \"sum\": 11007.742166519165, \"min\": 11007.742166519165}}, \"EndTime\": 1580272127.661869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272116.653683}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:47 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=57.2318638009 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:47 INFO 139797266573120] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:47 INFO 139797266573120] #quality_metric: host=algo-1, epoch=159, train loss <loss>=2.47750008106\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:47 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:49 INFO 139797266573120] Epoch[160] Batch[0] avg_epoch_loss=2.526800\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:49 INFO 139797266573120] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=2.52680039406\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:54 INFO 139797266573120] Epoch[160] Batch[5] avg_epoch_loss=2.527694\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:54 INFO 139797266573120] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=2.52769406637\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:54 INFO 139797266573120] Epoch[160] Batch [5]#011Speed: 62.34 samples/sec#011loss=2.527694\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:58 INFO 139797266573120] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10942.7649974823, \"sum\": 10942.7649974823, \"min\": 10942.7649974823}}, \"EndTime\": 1580272138.605049, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272127.661948}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:58 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=56.4751287449 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:58 INFO 139797266573120] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:58 INFO 139797266573120] #quality_metric: host=algo-1, epoch=160, train loss <loss>=2.54614808559\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:28:58 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:00 INFO 139797266573120] Epoch[161] Batch[0] avg_epoch_loss=2.649559\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:00 INFO 139797266573120] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=2.649559021\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:05 INFO 139797266573120] Epoch[161] Batch[5] avg_epoch_loss=2.507580\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:05 INFO 139797266573120] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=2.50758016109\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:05 INFO 139797266573120] Epoch[161] Batch [5]#011Speed: 62.22 samples/sec#011loss=2.507580\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] Epoch[161] Batch[10] avg_epoch_loss=2.506020\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=2.50414690971\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] Epoch[161] Batch [10]#011Speed: 61.42 samples/sec#011loss=2.504147\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12100.827932357788, \"sum\": 12100.827932357788, \"min\": 12100.827932357788}}, \"EndTime\": 1580272150.706447, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272138.605117}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] #throughput_metric: host=algo-1, train throughput=55.0370966393 records/second\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] #quality_metric: host=algo-1, epoch=161, train loss <loss>=2.50601959229\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] Loading parameters from best epoch (121)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 97.17988967895508, \"sum\": 97.17988967895508, \"min\": 97.17988967895508}}, \"EndTime\": 1580272150.804225, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272150.706516}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] stopping training now\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] Final loss: 2.43859718063 (occurred at epoch 121)\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] #quality_metric: host=algo-1, train final_loss <loss>=2.43859718063\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 WARNING 139797266573120] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 WARNING 139797266573120] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:10 INFO 139797266573120] All workers finished. Serializing model for prediction.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 32726.91798210144, \"sum\": 32726.91798210144, \"min\": 32726.91798210144}}, \"EndTime\": 1580272183.531955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272150.804291}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:44 INFO 139797266573120] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 33793.54906082153, \"sum\": 33793.54906082153, \"min\": 33793.54906082153}}, \"EndTime\": 1580272184.598534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272183.53377}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:44 INFO 139797266573120] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:44 INFO 139797266573120] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 149.9190330505371, \"sum\": 149.9190330505371, \"min\": 149.9190330505371}}, \"EndTime\": 1580272184.748554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272184.598592}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:44 INFO 139797266573120] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:29:44 INFO 139797266573120] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.04291534423828125, \"sum\": 0.04291534423828125, \"min\": 0.04291534423828125}}, \"EndTime\": 1580272184.749371, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272184.748602}\n",
      "\u001b[0m\n",
      "\n",
      "2020-01-29 04:30:05 Uploading - Uploading generated training model\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 16621.10686302185, \"sum\": 16621.10686302185, \"min\": 16621.10686302185}}, \"EndTime\": 1580272201.370439, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272184.749425}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, RMSE): 37.7798784258\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, mean_wQuantileLoss): 0.12909053\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.1]): 0.07106977\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.2]): 0.115571454\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.3]): 0.14268333\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.4]): 0.16004053\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.5]): 0.16657838\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.6]): 0.16287361\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.7]): 0.14747776\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.8]): 0.11830297\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #test_score (algo-1, wQuantileLoss[0.9]): 0.077216975\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.12909053266\u001b[0m\n",
      "\u001b[34m[01/29/2020 04:30:01 INFO 139797266573120] #quality_metric: host=algo-1, test RMSE <loss>=37.7798784258\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1943076.8299102783, \"sum\": 1943076.8299102783, \"min\": 1943076.8299102783}, \"setuptime\": {\"count\": 1, \"max\": 7.581949234008789, \"sum\": 7.581949234008789, \"min\": 7.581949234008789}}, \"EndTime\": 1580272201.67236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1580272201.370517}\n",
      "\u001b[0m\n",
      "\n",
      "2020-01-29 04:30:17 Completed - Training job completed\n",
      "Training seconds: 1993\n",
      "Billable seconds: 1993\n"
     ]
    }
   ],
   "source": [
    "# This step takes around 35 minutes to train the model with m4.xlarge instance\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard code name for now as we stopped the notebook.  \n",
    "# If you do this in a single sitting, you don't need to hard code\n",
    "# job_name = 'deepar-biketrain-with-categories-2018-12-21-04-05-44-478'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job name: deepar-biketrain-with-categories-2020-01-29-03-55-20-132\n"
     ]
    }
   ],
   "source": [
    "print ('job name: {0}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "# Create an endpoint for real-time predictions\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: deepar-biketrain-with-categories-2020-01-29-03-55-20-132\n"
     ]
    }
   ],
   "source": [
    "print ('endpoint name: {0}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to terminate the end point after completing the demo\n",
    "# Otherwise, you account will accumulate hourly charges\n",
    "\n",
    "# you can delete from sagemaker management console or through command line or throught code\n",
    "\n",
    "# sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
